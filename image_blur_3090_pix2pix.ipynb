{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기준size (512,512,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from tensorflow.keras import backend as K\n",
    "from keras import objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 64) 3072        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 128, 128, 64) 0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 64, 128)  131072      leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 64, 64, 128)  512         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 64, 64, 128)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 256)  524288      leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 256)  1024        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 32, 32, 256)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 16, 16, 512)  2097152     leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 16, 512)  2048        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 16, 16, 512)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 8, 8, 512)    4194304     leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 8, 8, 512)    2048        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 8, 8, 512)    0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 4, 4, 512)    4194304     leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 4, 4, 512)    2048        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 4, 4, 512)    0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 2, 2, 512)    4194304     leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 2, 2, 512)    2048        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 2, 2, 512)    0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 1, 1, 512)    4194304     leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 1, 1, 512)    2048        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 1, 1, 512)    0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 2, 2, 512)    4194304     leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 2, 2, 512)    2048        conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 2, 2, 512)    0           batch_normalization_7[0][0]      \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 2, 2, 512)    0           tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 2, 2, 512)    0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 4, 4, 512)    4194304     re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 4, 4, 512)    2048        conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 4, 4, 1024)   0           batch_normalization_8[0][0]      \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 4, 4, 1024)   0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 4, 4, 1024)   0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 8, 8, 512)    8388608     re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 512)    2048        conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (None, 8, 8, 512)    0           batch_normalization_9[0][0]      \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 8, 8, 512)    0           tf.__operators__.add_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, 8, 8, 512)    0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 16, 16, 512)  4194304     re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 512)  2048        conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_2 (TFOpLam (None, 16, 16, 512)  0           batch_normalization_10[0][0]     \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_3 (ReLU)                  (None, 16, 16, 512)  0           tf.__operators__.add_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 32, 32, 256)  2097152     re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 256)  1024        conv2d_transpose_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_3 (TFOpLam (None, 32, 32, 256)  0           batch_normalization_11[0][0]     \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_4 (ReLU)                  (None, 32, 32, 256)  0           tf.__operators__.add_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 64, 64, 128)  524288      re_lu_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 64, 64, 128)  512         conv2d_transpose_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_4 (TFOpLam (None, 64, 64, 128)  0           batch_normalization_12[0][0]     \n",
      "                                                                 batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_5 (ReLU)                  (None, 64, 64, 128)  0           tf.__operators__.add_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTrans (None, 128, 128, 64) 131072      re_lu_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 128, 128, 64) 256         conv2d_transpose_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_5 (TFOpLam (None, 128, 128, 64) 0           batch_normalization_13[0][0]     \n",
      "                                                                 leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_6 (ReLU)                  (None, 128, 128, 64) 0           tf.__operators__.add_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTrans (None, 256, 256, 3)  3075        re_lu_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 43,281,667\n",
      "Trainable params: 43,270,787\n",
      "Non-trainable params: 10,880\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Conv2D,BatchNormalization,Dropout,Activation,LeakyReLU,UpSampling2D,Input,Dense,Reshape,Flatten,Conv2DTranspose,ReLU,concatenate,ZeroPadding2D\n",
    "import numpy as np\n",
    "\n",
    "initializer = tf.random_normal_initializer(0.,0.02)\n",
    "\n",
    "inputs = Input(shape=(256,256,3))\n",
    "layer1 = Conv2D(filters=64,kernel_size=4,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(inputs)\n",
    "layer1 = LeakyReLU()(layer1)\n",
    "layer1_ = layer1\n",
    "\n",
    "layer2 = Conv2D(filters=128,kernel_size=4,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(layer1)\n",
    "layer2_ = BatchNormalization()(layer2)\n",
    "layer2 = LeakyReLU()(layer2_)\n",
    "\n",
    "layer3 = Conv2D(filters=256,kernel_size=4,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(layer2)\n",
    "layer3_ = BatchNormalization()(layer3)\n",
    "layer3 = LeakyReLU()(layer3_)\n",
    "\n",
    "layer4 = Conv2D(filters=512,kernel_size=4,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(layer3)\n",
    "layer4_ = BatchNormalization()(layer4)\n",
    "layer4 = LeakyReLU()(layer4_)\n",
    "\n",
    "layer5 = Conv2D(filters=512,kernel_size=4,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(layer4)\n",
    "layer5_ = BatchNormalization()(layer5)\n",
    "layer5 = LeakyReLU()(layer5_)\n",
    "\n",
    "layer6 = Conv2D(filters=512,kernel_size=4,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(layer5)\n",
    "layer6_ = BatchNormalization()(layer6)\n",
    "layer6 = LeakyReLU()(layer6_)\n",
    "\n",
    "layer7 = Conv2D(filters=512,kernel_size=4,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(layer6)\n",
    "layer7_ = BatchNormalization()(layer7)\n",
    "layer7 = LeakyReLU()(layer7_)\n",
    "\n",
    "\n",
    "\n",
    "layer8 = Conv2D(filters=512,kernel_size=4,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(layer7)\n",
    "layer8_ = BatchNormalization()(layer8)\n",
    "layer8 = LeakyReLU()(layer8_)\n",
    "\n",
    "\n",
    "\n",
    "layer9 = Conv2DTranspose(filters=512,kernel_size=4,strides=2,padding='same',kernel_initializer=initializer,use_bias=False)(layer8)\n",
    "layer9 = BatchNormalization()(layer9)\n",
    "layer9 = layer9+layer7_\n",
    "layer9 = Dropout(0.5)(layer9)\n",
    "layer9 = ReLU()(layer9)\n",
    "\n",
    "layer10 = Conv2DTranspose(filters=512,kernel_size=4,strides=2,padding='same',kernel_initializer=initializer,use_bias=False)(layer9)\n",
    "layer10 = BatchNormalization()(layer10)\n",
    "layer10 = concatenate([layer10,layer6_])\n",
    "layer10 = Dropout(0.5)(layer10)\n",
    "layer10 = ReLU()(layer10)\n",
    "\n",
    "layer11 = Conv2DTranspose(filters=512,kernel_size=4,strides=2,padding='same',kernel_initializer=initializer,use_bias=False)(layer10)\n",
    "layer11 = BatchNormalization()(layer11)\n",
    "layer11 = layer11+layer5_\n",
    "layer11 = Dropout(0.5)(layer11)\n",
    "layer11 = ReLU()(layer11)\n",
    "                      \n",
    "layer12 = Conv2DTranspose(filters=512,kernel_size=4,strides=2,padding='same',kernel_initializer=initializer,use_bias=False)(layer11)\n",
    "layer12 = BatchNormalization()(layer12)\n",
    "layer12 = layer12+layer4_\n",
    "layer12 = ReLU()(layer12)\n",
    "                      \n",
    "layer13 = Conv2DTranspose(filters=256,kernel_size=4,strides=2,padding='same',kernel_initializer=initializer,use_bias=False)(layer12)\n",
    "layer13 = BatchNormalization()(layer13)\n",
    "layer13 = layer13+layer3_\n",
    "layer13 = ReLU()(layer13)\n",
    "\n",
    "layer14 = Conv2DTranspose(filters=128,kernel_size=4,strides=2,padding='same',kernel_initializer=initializer,use_bias=False)(layer13)\n",
    "layer14 = BatchNormalization()(layer14)\n",
    "layer14 = layer14+layer2_\n",
    "layer14 = ReLU()(layer14)\n",
    "\n",
    "layer15 = Conv2DTranspose(filters=64,kernel_size=4,strides=2,padding='same',kernel_initializer=initializer,use_bias=False)(layer14)\n",
    "layer15 = BatchNormalization()(layer15)\n",
    "layer15 = layer15+layer1_\n",
    "layer15 = ReLU()(layer15)\n",
    "\n",
    "outputs = Conv2DTranspose(3,4,strides=2,padding='same',kernel_initializer=initializer,activation='tanh')(layer15)\n",
    "\n",
    "Generator = Model(inputs=inputs,outputs=outputs)\n",
    "Generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17857, 256, 256, 3), (17857, 256, 256, 3))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npy_x = ((np.load(\"G:/내 드라이브/P_project/\"+\"datasets/npy/npy_x_256_6_color.npy\").reshape(-1,256,256,3)/255.0)-0.5)/0.5\n",
    "npy_y = ((np.load(\"G:/내 드라이브/P_project/\"+\"datasets/npy/npy_y_256_6_color.npy\")/255.0)-0.5)/0.5#[:,int(npy_x.shape[1]/4):3*int(npy_x.shape[1]/4),int(npy_x.shape[2]/4):3*int(npy_x.shape[2]/4),:]\n",
    "npy_x.shape,npy_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pix2pix_loss(y_true, y_pred):\n",
    "    y_true_flat = K.batch_flatten(y_true)\n",
    "    y_pred_flat = K.batch_flatten(y_pred)\n",
    "\n",
    "    # Adversarial Loss\n",
    "    L_adv = objectives.binary_crossentropy(y_true_flat, y_pred_flat)\n",
    "\n",
    "    # A to B loss\n",
    "    # b_flat = K.batch_flatten(b)\n",
    "    # bp_flat = K.batch_flatten(bp)\n",
    "    L_atob = K.mean(K.abs(y_true_flat - y_pred_flat))\n",
    "\n",
    "    return L_adv + 50 * L_atob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_standard(y_true,y_pred):\n",
    "    return tf.math.reduce_mean(tf.abs(y_true-y_pred))-tf.math.reduce_std(y_pred)/8\n",
    "def mse_standard(y_true,y_pred):\n",
    "    return tf.math.reduce_mean(tf.square(y_true-y_pred))-tf.math.reduce_std(y_pred)/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,BatchNormalization,Dense,Input,concatenate,Reshape,Flatten,AveragePooling2D,Conv2DTranspose\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "optimizer = Adam(lr=0.003)\n",
    "modelpath = \"./datasets/models/pix2pix_myloss_color.h5\"\n",
    "cp = ModelCheckpoint(monitor = 'val_loss',filepath = modelpath,save_best_only=True)\n",
    "es = EarlyStopping(monitor = \"val_loss\",patience=50)\n",
    "reLR = ReduceLROnPlateau(monitor = 'val_loss',patience=13,factor=0.6)\n",
    "\n",
    "\n",
    "Generator.compile(loss = pix2pix_loss,optimizer = optimizer)\n",
    "Generator.fit(npy_x,npy_y,epochs=1000,validation_split=0.15,batch_size=16,callbacks=[cp,es,reLR])\n",
    "Generator.summary()\n",
    "\n",
    "\n",
    "'868'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "238/238 [==============================] - 184s 703ms/step - loss: 0.1666 - mae: 0.2404 - val_loss: 0.8531 - val_mae: 0.9639\n",
      "Epoch 2/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: 0.0668 - mae: 0.1454 - val_loss: 0.8186 - val_mae: 0.9353\n",
      "Epoch 3/1000\n",
      "238/238 [==============================] - 102s 430ms/step - loss: 0.0336 - mae: 0.1132 - val_loss: 0.8205 - val_mae: 0.9113\n",
      "Epoch 4/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: 0.0229 - mae: 0.1027 - val_loss: 0.8497 - val_mae: 0.9730\n",
      "Epoch 5/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: 0.0158 - mae: 0.0953 - val_loss: 0.8098 - val_mae: 0.9296\n",
      "Epoch 6/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: 0.0141 - mae: 0.0938 - val_loss: 0.8207 - val_mae: 0.9441\n",
      "Epoch 7/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: 0.0113 - mae: 0.0912 - val_loss: 1.0070 - val_mae: 1.0786\n",
      "Epoch 8/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: 0.0043 - mae: 0.0839 - val_loss: 0.8263 - val_mae: 0.8624\n",
      "Epoch 9/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0054 - mae: 0.0746 - val_loss: 1.0828 - val_mae: 1.1112\n",
      "Epoch 10/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0087 - mae: 0.0712 - val_loss: 0.9012 - val_mae: 0.9982\n",
      "Epoch 11/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0087 - mae: 0.0713 - val_loss: 1.0147 - val_mae: 1.0774\n",
      "Epoch 12/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0108 - mae: 0.0688 - val_loss: 1.1343 - val_mae: 1.1343\n",
      "Epoch 13/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0140 - mae: 0.0657 - val_loss: 1.1343 - val_mae: 1.1343\n",
      "Epoch 14/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0145 - mae: 0.0654 - val_loss: 1.1342 - val_mae: 1.1343\n",
      "Epoch 15/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0158 - mae: 0.0638 - val_loss: 0.8124 - val_mae: 0.8781\n",
      "Epoch 16/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0177 - mae: 0.0621 - val_loss: 1.1273 - val_mae: 1.1340\n",
      "Epoch 17/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0166 - mae: 0.0629 - val_loss: 1.1343 - val_mae: 1.1343\n",
      "Epoch 18/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0179 - mae: 0.0618 - val_loss: 0.8760 - val_mae: 0.9678\n",
      "Epoch 19/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0165 - mae: 0.0630 - val_loss: 0.7824 - val_mae: 0.8711\n",
      "Epoch 20/1000\n",
      "238/238 [==============================] - 102s 431ms/step - loss: -0.0176 - mae: 0.0620 - val_loss: 1.1343 - val_mae: 1.1343\n",
      "Epoch 21/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0185 - mae: 0.0610 - val_loss: 0.9822 - val_mae: 1.0588\n",
      "Epoch 22/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0220 - mae: 0.0579 - val_loss: 1.1267 - val_mae: 1.1347\n",
      "Epoch 23/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0209 - mae: 0.0588 - val_loss: 0.4158 - val_mae: 0.5251\n",
      "Epoch 24/1000\n",
      "238/238 [==============================] - 102s 430ms/step - loss: -0.0193 - mae: 0.0605 - val_loss: 1.1342 - val_mae: 1.1343\n",
      "Epoch 25/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0226 - mae: 0.0572 - val_loss: 0.7300 - val_mae: 0.8364\n",
      "Epoch 26/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0219 - mae: 0.0581 - val_loss: 0.9462 - val_mae: 1.0038\n",
      "Epoch 27/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0234 - mae: 0.0563 - val_loss: 1.1271 - val_mae: 1.1329\n",
      "Epoch 28/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0240 - mae: 0.0557 - val_loss: 1.1342 - val_mae: 1.1343\n",
      "Epoch 29/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0256 - mae: 0.0542 - val_loss: 1.1153 - val_mae: 1.1320\n",
      "Epoch 30/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0257 - mae: 0.0541 - val_loss: 1.0476 - val_mae: 1.0821\n",
      "Epoch 31/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0255 - mae: 0.0540 - val_loss: 1.0369 - val_mae: 1.0730\n",
      "Epoch 32/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0254 - mae: 0.0546 - val_loss: 0.6229 - val_mae: 0.7390\n",
      "Epoch 33/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0230 - mae: 0.0565 - val_loss: 1.1170 - val_mae: 1.1335\n",
      "Epoch 34/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0269 - mae: 0.0528 - val_loss: 1.0789 - val_mae: 1.1138\n",
      "Epoch 35/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0272 - mae: 0.0525 - val_loss: 0.5921 - val_mae: 0.6791\n",
      "Epoch 36/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0276 - mae: 0.0523 - val_loss: 0.9458 - val_mae: 1.0061\n",
      "Epoch 37/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0283 - mae: 0.0514 - val_loss: 0.7774 - val_mae: 0.8228\n",
      "Epoch 38/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0292 - mae: 0.0506 - val_loss: 0.6467 - val_mae: 0.7293\n",
      "Epoch 39/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0297 - mae: 0.0501 - val_loss: 1.1245 - val_mae: 1.1321\n",
      "Epoch 40/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0304 - mae: 0.0495 - val_loss: 0.7827 - val_mae: 0.8654\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.001800000015646219.\n",
      "Epoch 41/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0331 - mae: 0.0468 - val_loss: 1.1224 - val_mae: 1.1322\n",
      "Epoch 42/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0332 - mae: 0.0460 - val_loss: 0.6711 - val_mae: 0.7570\n",
      "Epoch 43/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0332 - mae: 0.0464 - val_loss: 0.5649 - val_mae: 0.6388\n",
      "Epoch 44/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0343 - mae: 0.0456 - val_loss: 1.0770 - val_mae: 1.1158\n",
      "Epoch 45/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0329 - mae: 0.0471 - val_loss: 1.1289 - val_mae: 1.1330\n",
      "Epoch 46/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0331 - mae: 0.0466 - val_loss: 0.6305 - val_mae: 0.7223\n",
      "Epoch 47/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0340 - mae: 0.0456 - val_loss: 0.8119 - val_mae: 0.8953\n",
      "Epoch 48/1000\n",
      "238/238 [==============================] - 103s 434ms/step - loss: -0.0346 - mae: 0.0452 - val_loss: 0.3341 - val_mae: 0.4388\n",
      "Epoch 49/1000\n",
      "238/238 [==============================] - 102s 431ms/step - loss: -0.0331 - mae: 0.0467 - val_loss: 0.8657 - val_mae: 0.8657\n",
      "Epoch 50/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0335 - mae: 0.0461 - val_loss: 0.8424 - val_mae: 0.8573\n",
      "Epoch 51/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0350 - mae: 0.0448 - val_loss: 1.1039 - val_mae: 1.1212\n",
      "Epoch 52/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0345 - mae: 0.0451 - val_loss: 1.1268 - val_mae: 1.1329\n",
      "Epoch 53/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0355 - mae: 0.0442 - val_loss: 1.0628 - val_mae: 1.0852\n",
      "Epoch 54/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0367 - mae: 0.0433 - val_loss: 0.6106 - val_mae: 0.6890\n",
      "Epoch 55/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0368 - mae: 0.0429 - val_loss: 0.8019 - val_mae: 0.8724\n",
      "Epoch 56/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0362 - mae: 0.0434 - val_loss: 0.5451 - val_mae: 0.6534\n",
      "Epoch 57/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0367 - mae: 0.0430 - val_loss: 0.3560 - val_mae: 0.4628\n",
      "Epoch 58/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0371 - mae: 0.0427 - val_loss: 1.0469 - val_mae: 1.0877\n",
      "Epoch 59/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0376 - mae: 0.0423 - val_loss: 0.8657 - val_mae: 0.8657\n",
      "Epoch 60/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0369 - mae: 0.0427 - val_loss: 0.4268 - val_mae: 0.5183\n",
      "Epoch 61/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0378 - mae: 0.0420 - val_loss: 0.5338 - val_mae: 0.6216\n",
      "Epoch 62/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0376 - mae: 0.0422 - val_loss: 0.9273 - val_mae: 0.9874\n",
      "Epoch 63/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0374 - mae: 0.0421 - val_loss: 1.0857 - val_mae: 1.1097\n",
      "Epoch 64/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0382 - mae: 0.0415 - val_loss: 0.6795 - val_mae: 0.7365\n",
      "Epoch 65/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0384 - mae: 0.0413 - val_loss: 0.8612 - val_mae: 0.9194\n",
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 0.0010799999814480542.\n",
      "Epoch 66/1000\n",
      "238/238 [==============================] - 103s 434ms/step - loss: -0.0401 - mae: 0.0396 - val_loss: 0.7656 - val_mae: 0.8378\n",
      "Epoch 67/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0402 - mae: 0.0395 - val_loss: 0.7277 - val_mae: 0.8035\n",
      "Epoch 68/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0398 - mae: 0.0396 - val_loss: 0.5391 - val_mae: 0.6311\n",
      "Epoch 69/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0405 - mae: 0.0392 - val_loss: 1.1129 - val_mae: 1.1229\n",
      "Epoch 70/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0407 - mae: 0.0389 - val_loss: 0.4568 - val_mae: 0.5440\n",
      "Epoch 71/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0410 - mae: 0.0386 - val_loss: 0.7482 - val_mae: 0.8187\n",
      "Epoch 72/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0397 - mae: 0.0399 - val_loss: 0.4139 - val_mae: 0.5244\n",
      "Epoch 73/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0409 - mae: 0.0388 - val_loss: 0.6778 - val_mae: 0.7491\n",
      "Epoch 74/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0414 - mae: 0.0383 - val_loss: 0.7487 - val_mae: 0.8232\n",
      "Epoch 75/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0415 - mae: 0.0386 - val_loss: 0.1531 - val_mae: 0.2325\n",
      "Epoch 76/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0405 - mae: 0.0391 - val_loss: 0.8375 - val_mae: 0.8550\n",
      "Epoch 77/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0416 - mae: 0.0383 - val_loss: 0.9755 - val_mae: 1.0209\n",
      "Epoch 78/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0410 - mae: 0.0387 - val_loss: 0.6208 - val_mae: 0.6788\n",
      "Epoch 79/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0411 - mae: 0.0386 - val_loss: 0.1755 - val_mae: 0.2592\n",
      "Epoch 80/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0411 - mae: 0.0386 - val_loss: 0.6965 - val_mae: 0.7763\n",
      "Epoch 81/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0404 - mae: 0.0393 - val_loss: 0.8573 - val_mae: 0.8639\n",
      "Epoch 82/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0421 - mae: 0.0378 - val_loss: 0.6155 - val_mae: 0.6982\n",
      "Epoch 83/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0416 - mae: 0.0382 - val_loss: 0.7545 - val_mae: 0.8054\n",
      "Epoch 84/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0424 - mae: 0.0376 - val_loss: 0.6662 - val_mae: 0.7519\n",
      "Epoch 85/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0417 - mae: 0.0379 - val_loss: 0.3829 - val_mae: 0.4810\n",
      "Epoch 86/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0422 - mae: 0.0375 - val_loss: 0.6310 - val_mae: 0.7247\n",
      "Epoch 87/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0423 - mae: 0.0374 - val_loss: 0.8318 - val_mae: 0.8506\n",
      "Epoch 88/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0423 - mae: 0.0373 - val_loss: 1.0308 - val_mae: 1.0614\n",
      "Epoch 89/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0421 - mae: 0.0377 - val_loss: 0.4860 - val_mae: 0.5576\n",
      "Epoch 90/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0424 - mae: 0.0374 - val_loss: 0.2847 - val_mae: 0.3789\n",
      "Epoch 91/1000\n",
      "238/238 [==============================] - 103s 434ms/step - loss: -0.0421 - mae: 0.0374 - val_loss: 0.6061 - val_mae: 0.6869\n",
      "Epoch 92/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0417 - mae: 0.0382 - val_loss: 0.6670 - val_mae: 0.7423\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 0.0006479999748989939.\n",
      "Epoch 93/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0431 - mae: 0.0365 - val_loss: 0.1461 - val_mae: 0.2237\n",
      "Epoch 94/1000\n",
      "238/238 [==============================] - 102s 430ms/step - loss: -0.0435 - mae: 0.0361 - val_loss: 0.6042 - val_mae: 0.7008\n",
      "Epoch 95/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0431 - mae: 0.0365 - val_loss: 0.8168 - val_mae: 0.8387\n",
      "Epoch 96/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0430 - mae: 0.0366 - val_loss: 0.4289 - val_mae: 0.5299\n",
      "Epoch 97/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0416 - mae: 0.0381 - val_loss: 0.3739 - val_mae: 0.4779\n",
      "Epoch 98/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0433 - mae: 0.0362 - val_loss: 0.3944 - val_mae: 0.4878\n",
      "Epoch 99/1000\n",
      "238/238 [==============================] - 103s 434ms/step - loss: -0.0436 - mae: 0.0359 - val_loss: 0.2794 - val_mae: 0.3688\n",
      "Epoch 100/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0434 - mae: 0.0361 - val_loss: 0.3970 - val_mae: 0.4843\n",
      "Epoch 101/1000\n",
      "238/238 [==============================] - 103s 434ms/step - loss: -0.0435 - mae: 0.0360 - val_loss: 0.4005 - val_mae: 0.4870\n",
      "Epoch 102/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0439 - mae: 0.0358 - val_loss: 0.3986 - val_mae: 0.4998\n",
      "Epoch 103/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0439 - mae: 0.0360 - val_loss: 0.2642 - val_mae: 0.3640\n",
      "Epoch 104/1000\n",
      "238/238 [==============================] - 103s 434ms/step - loss: -0.0441 - mae: 0.0357 - val_loss: 0.5485 - val_mae: 0.6355\n",
      "Epoch 105/1000\n",
      "238/238 [==============================] - 103s 434ms/step - loss: -0.0438 - mae: 0.0359 - val_loss: 0.5539 - val_mae: 0.6244\n",
      "Epoch 106/1000\n",
      "238/238 [==============================] - 103s 434ms/step - loss: -0.0440 - mae: 0.0356 - val_loss: 0.9281 - val_mae: 0.9691\n",
      "Epoch 107/1000\n",
      "238/238 [==============================] - 103s 434ms/step - loss: -0.0436 - mae: 0.0359 - val_loss: 0.4251 - val_mae: 0.5225\n",
      "Epoch 108/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0434 - mae: 0.0363 - val_loss: 0.1029 - val_mae: 0.1843\n",
      "Epoch 109/1000\n",
      "238/238 [==============================] - 103s 431ms/step - loss: -0.0434 - mae: 0.0360 - val_loss: 0.3834 - val_mae: 0.4925\n",
      "Epoch 110/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0441 - mae: 0.0355 - val_loss: 0.6290 - val_mae: 0.6842\n",
      "Epoch 111/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0440 - mae: 0.0357 - val_loss: 0.6796 - val_mae: 0.7375\n",
      "Epoch 112/1000\n",
      "238/238 [==============================] - 103s 434ms/step - loss: -0.0441 - mae: 0.0354 - val_loss: 0.2111 - val_mae: 0.3010\n",
      "Epoch 113/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0440 - mae: 0.0355 - val_loss: 0.4528 - val_mae: 0.5453\n",
      "Epoch 114/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0438 - mae: 0.0357 - val_loss: 0.3798 - val_mae: 0.4800\n",
      "Epoch 115/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0448 - mae: 0.0351 - val_loss: 0.6451 - val_mae: 0.7091\n",
      "Epoch 116/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0445 - mae: 0.0351 - val_loss: 0.5835 - val_mae: 0.6584\n",
      "Epoch 117/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0443 - mae: 0.0354 - val_loss: 0.5855 - val_mae: 0.6531\n",
      "Epoch 118/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0442 - mae: 0.0354 - val_loss: 0.6575 - val_mae: 0.7287\n",
      "Epoch 119/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0445 - mae: 0.0351 - val_loss: 0.4444 - val_mae: 0.5474\n",
      "Epoch 120/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0447 - mae: 0.0350 - val_loss: 0.3579 - val_mae: 0.4633\n",
      "Epoch 121/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0441 - mae: 0.0355 - val_loss: 0.3591 - val_mae: 0.4585\n",
      "Epoch 122/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0435 - mae: 0.0361 - val_loss: 0.7717 - val_mae: 0.8709\n",
      "Epoch 123/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0446 - mae: 0.0351 - val_loss: 0.3308 - val_mae: 0.4350\n",
      "Epoch 124/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0446 - mae: 0.0349 - val_loss: 0.8275 - val_mae: 0.8484\n",
      "Epoch 125/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0445 - mae: 0.0350 - val_loss: 0.6377 - val_mae: 0.6949\n",
      "\n",
      "Epoch 00125: ReduceLROnPlateau reducing learning rate to 0.0003887999919243157.\n",
      "Epoch 126/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0454 - mae: 0.0342 - val_loss: 0.1838 - val_mae: 0.2685\n",
      "Epoch 127/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0452 - mae: 0.0343 - val_loss: 0.1113 - val_mae: 0.1926\n",
      "Epoch 128/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0455 - mae: 0.0342 - val_loss: 0.1490 - val_mae: 0.2325\n",
      "Epoch 129/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0451 - mae: 0.0342 - val_loss: 0.5272 - val_mae: 0.6139\n",
      "Epoch 130/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0456 - mae: 0.0342 - val_loss: 0.2044 - val_mae: 0.2921\n",
      "Epoch 131/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0453 - mae: 0.0340 - val_loss: 0.3112 - val_mae: 0.3925\n",
      "Epoch 132/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0455 - mae: 0.0341 - val_loss: 0.2354 - val_mae: 0.3150\n",
      "Epoch 133/1000\n",
      "238/238 [==============================] - 103s 434ms/step - loss: -0.0453 - mae: 0.0342 - val_loss: 0.5591 - val_mae: 0.6499\n",
      "Epoch 134/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0455 - mae: 0.0340 - val_loss: 0.6024 - val_mae: 0.6644\n",
      "Epoch 135/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0458 - mae: 0.0339 - val_loss: 0.1713 - val_mae: 0.2519\n",
      "Epoch 136/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0457 - mae: 0.0339 - val_loss: 0.1865 - val_mae: 0.2784\n",
      "Epoch 137/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0461 - mae: 0.0338 - val_loss: 0.5293 - val_mae: 0.5997\n",
      "Epoch 138/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0456 - mae: 0.0339 - val_loss: 0.2356 - val_mae: 0.3090\n",
      "Epoch 139/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0455 - mae: 0.0340 - val_loss: 0.5160 - val_mae: 0.6011\n",
      "Epoch 140/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0459 - mae: 0.0338 - val_loss: 0.2694 - val_mae: 0.3533\n",
      "Epoch 141/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0456 - mae: 0.0340 - val_loss: 0.2928 - val_mae: 0.3986\n",
      "Epoch 142/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0460 - mae: 0.0336 - val_loss: 0.3400 - val_mae: 0.4216\n",
      "\n",
      "Epoch 00142: ReduceLROnPlateau reducing learning rate to 0.0002332799951545894.\n",
      "Epoch 143/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0459 - mae: 0.0338 - val_loss: 0.2494 - val_mae: 0.3270\n",
      "Epoch 144/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0465 - mae: 0.0333 - val_loss: 0.2604 - val_mae: 0.3409\n",
      "Epoch 145/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0464 - mae: 0.0331 - val_loss: 0.3165 - val_mae: 0.4202\n",
      "Epoch 146/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0462 - mae: 0.0336 - val_loss: 0.2337 - val_mae: 0.3174\n",
      "Epoch 147/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0466 - mae: 0.0331 - val_loss: 0.2047 - val_mae: 0.2964\n",
      "Epoch 148/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0460 - mae: 0.0334 - val_loss: 0.3616 - val_mae: 0.4403\n",
      "Epoch 149/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0465 - mae: 0.0332 - val_loss: 0.1276 - val_mae: 0.2062\n",
      "Epoch 150/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0461 - mae: 0.0333 - val_loss: 0.1843 - val_mae: 0.2612\n",
      "Epoch 151/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0462 - mae: 0.0335 - val_loss: 0.2098 - val_mae: 0.2792\n",
      "Epoch 152/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0463 - mae: 0.0333 - val_loss: 0.1952 - val_mae: 0.2785\n",
      "Epoch 153/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0464 - mae: 0.0333 - val_loss: 0.4404 - val_mae: 0.5344\n",
      "Epoch 154/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0464 - mae: 0.0331 - val_loss: 0.2647 - val_mae: 0.3670\n",
      "Epoch 155/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0464 - mae: 0.0332 - val_loss: 0.1210 - val_mae: 0.1986\n",
      "Epoch 156/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0463 - mae: 0.0334 - val_loss: 0.5735 - val_mae: 0.6457\n",
      "Epoch 157/1000\n",
      "238/238 [==============================] - 103s 434ms/step - loss: -0.0463 - mae: 0.0332 - val_loss: 0.2882 - val_mae: 0.3740\n",
      "Epoch 158/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0461 - mae: 0.0335 - val_loss: 0.1259 - val_mae: 0.2060\n",
      "Epoch 159/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0464 - mae: 0.0332 - val_loss: 0.2078 - val_mae: 0.2800\n",
      "\n",
      "Epoch 00159: ReduceLROnPlateau reducing learning rate to 0.00013996799534652383.\n",
      "Epoch 160/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0466 - mae: 0.0331 - val_loss: 0.1474 - val_mae: 0.2229\n",
      "Epoch 161/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0466 - mae: 0.0328 - val_loss: 0.1174 - val_mae: 0.1928\n",
      "Epoch 162/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0463 - mae: 0.0332 - val_loss: 0.0823 - val_mae: 0.1566\n",
      "Epoch 163/1000\n",
      "238/238 [==============================] - 103s 431ms/step - loss: -0.0468 - mae: 0.0329 - val_loss: 0.2672 - val_mae: 0.3410\n",
      "Epoch 164/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0464 - mae: 0.0331 - val_loss: 0.1526 - val_mae: 0.2417\n",
      "Epoch 165/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0467 - mae: 0.0330 - val_loss: 0.2558 - val_mae: 0.3505\n",
      "Epoch 166/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0473 - mae: 0.0327 - val_loss: 0.2795 - val_mae: 0.3649\n",
      "Epoch 167/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0467 - mae: 0.0329 - val_loss: 0.3088 - val_mae: 0.3919\n",
      "Epoch 168/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0465 - mae: 0.0329 - val_loss: 0.2376 - val_mae: 0.3272\n",
      "Epoch 169/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0470 - mae: 0.0329 - val_loss: 0.0451 - val_mae: 0.1189\n",
      "Epoch 170/1000\n",
      "238/238 [==============================] - 102s 429ms/step - loss: -0.0466 - mae: 0.0330 - val_loss: 0.1275 - val_mae: 0.2114\n",
      "Epoch 171/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0470 - mae: 0.0327 - val_loss: 0.1545 - val_mae: 0.2359\n",
      "Epoch 172/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0469 - mae: 0.0328 - val_loss: 0.1034 - val_mae: 0.1791\n",
      "Epoch 173/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0469 - mae: 0.0328 - val_loss: 0.1981 - val_mae: 0.2934\n",
      "Epoch 174/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0466 - mae: 0.0330 - val_loss: 0.0949 - val_mae: 0.1725\n",
      "Epoch 175/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0466 - mae: 0.0329 - val_loss: 0.2572 - val_mae: 0.3249\n",
      "Epoch 176/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0464 - mae: 0.0332 - val_loss: 0.1713 - val_mae: 0.2547\n",
      "Epoch 177/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0471 - mae: 0.0327 - val_loss: 0.2873 - val_mae: 0.3682\n",
      "Epoch 178/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0469 - mae: 0.0328 - val_loss: 0.0940 - val_mae: 0.1665\n",
      "Epoch 179/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0473 - mae: 0.0325 - val_loss: 0.0434 - val_mae: 0.1171\n",
      "Epoch 180/1000\n",
      "238/238 [==============================] - 102s 430ms/step - loss: -0.0468 - mae: 0.0330 - val_loss: 0.2467 - val_mae: 0.3451\n",
      "Epoch 181/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0467 - mae: 0.0329 - val_loss: 0.0940 - val_mae: 0.1658\n",
      "Epoch 182/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0472 - mae: 0.0327 - val_loss: 0.1441 - val_mae: 0.2272\n",
      "Epoch 183/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0477 - mae: 0.0323 - val_loss: 0.0996 - val_mae: 0.1748\n",
      "Epoch 184/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0470 - mae: 0.0328 - val_loss: 0.1431 - val_mae: 0.2240\n",
      "Epoch 185/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0469 - mae: 0.0329 - val_loss: 0.0783 - val_mae: 0.1549\n",
      "Epoch 186/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0469 - mae: 0.0328 - val_loss: 0.2555 - val_mae: 0.3441\n",
      "Epoch 187/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0471 - mae: 0.0325 - val_loss: 0.1091 - val_mae: 0.1875\n",
      "Epoch 188/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0472 - mae: 0.0325 - val_loss: 0.3957 - val_mae: 0.4601\n",
      "Epoch 189/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0471 - mae: 0.0327 - val_loss: 0.0578 - val_mae: 0.1341\n",
      "Epoch 190/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0468 - mae: 0.0328 - val_loss: 0.1181 - val_mae: 0.2043\n",
      "Epoch 191/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0472 - mae: 0.0325 - val_loss: 0.2493 - val_mae: 0.3451\n",
      "Epoch 192/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0471 - mae: 0.0326 - val_loss: 0.3072 - val_mae: 0.3797\n",
      "Epoch 193/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0469 - mae: 0.0328 - val_loss: 0.3030 - val_mae: 0.3971\n",
      "Epoch 194/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0472 - mae: 0.0326 - val_loss: 0.1330 - val_mae: 0.2197\n",
      "Epoch 195/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0471 - mae: 0.0324 - val_loss: 0.1804 - val_mae: 0.2712\n",
      "Epoch 196/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0472 - mae: 0.0325 - val_loss: 0.1975 - val_mae: 0.2891\n",
      "\n",
      "Epoch 00196: ReduceLROnPlateau reducing learning rate to 8.398079371545464e-05.\n",
      "Epoch 197/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0470 - mae: 0.0326 - val_loss: 0.0356 - val_mae: 0.1087\n",
      "Epoch 198/1000\n",
      "238/238 [==============================] - 103s 431ms/step - loss: -0.0470 - mae: 0.0325 - val_loss: 0.1725 - val_mae: 0.2430\n",
      "Epoch 199/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0474 - mae: 0.0324 - val_loss: 0.0090 - val_mae: 0.0810\n",
      "Epoch 200/1000\n",
      "238/238 [==============================] - 103s 431ms/step - loss: -0.0471 - mae: 0.0325 - val_loss: 0.1000 - val_mae: 0.1808\n",
      "Epoch 201/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0470 - mae: 0.0324 - val_loss: 0.0376 - val_mae: 0.1145\n",
      "Epoch 202/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0474 - mae: 0.0324 - val_loss: 0.0968 - val_mae: 0.1765\n",
      "Epoch 203/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0474 - mae: 0.0324 - val_loss: 0.1852 - val_mae: 0.2808\n",
      "Epoch 204/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0475 - mae: 0.0322 - val_loss: 0.0970 - val_mae: 0.1699\n",
      "Epoch 205/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0473 - mae: 0.0325 - val_loss: 0.1327 - val_mae: 0.2056\n",
      "Epoch 206/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0471 - mae: 0.0325 - val_loss: 0.0578 - val_mae: 0.1325\n",
      "Epoch 207/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0471 - mae: 0.0325 - val_loss: 0.0487 - val_mae: 0.1204\n",
      "Epoch 208/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0471 - mae: 0.0325 - val_loss: 0.1392 - val_mae: 0.2272\n",
      "Epoch 209/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0473 - mae: 0.0321 - val_loss: 0.2121 - val_mae: 0.3077\n",
      "Epoch 210/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0472 - mae: 0.0326 - val_loss: 0.1178 - val_mae: 0.2012\n",
      "Epoch 211/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0472 - mae: 0.0323 - val_loss: 0.1021 - val_mae: 0.1736\n",
      "Epoch 212/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0471 - mae: 0.0323 - val_loss: 0.1367 - val_mae: 0.2176\n",
      "Epoch 213/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0474 - mae: 0.0325 - val_loss: 0.1295 - val_mae: 0.2010\n",
      "Epoch 214/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0472 - mae: 0.0325 - val_loss: 0.0719 - val_mae: 0.1456\n",
      "Epoch 215/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0474 - mae: 0.0325 - val_loss: 0.1219 - val_mae: 0.2090\n",
      "Epoch 216/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0475 - mae: 0.0322 - val_loss: 0.0908 - val_mae: 0.1741\n",
      "\n",
      "Epoch 00216: ReduceLROnPlateau reducing learning rate to 5.038847448304296e-05.\n",
      "Epoch 217/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0473 - mae: 0.0324 - val_loss: 0.0848 - val_mae: 0.1644\n",
      "Epoch 218/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0474 - mae: 0.0323 - val_loss: 0.0546 - val_mae: 0.1295\n",
      "Epoch 219/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0471 - mae: 0.0325 - val_loss: 0.0672 - val_mae: 0.1417\n",
      "Epoch 220/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0474 - mae: 0.0323 - val_loss: 0.1090 - val_mae: 0.1969\n",
      "Epoch 221/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0477 - mae: 0.0321 - val_loss: 0.0770 - val_mae: 0.1543\n",
      "Epoch 222/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0472 - mae: 0.0322 - val_loss: 0.1696 - val_mae: 0.2591\n",
      "Epoch 223/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0471 - mae: 0.0324 - val_loss: 0.0721 - val_mae: 0.1561\n",
      "Epoch 224/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0471 - mae: 0.0323 - val_loss: 0.0624 - val_mae: 0.1378\n",
      "Epoch 225/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0473 - mae: 0.0322 - val_loss: 0.0497 - val_mae: 0.1288\n",
      "Epoch 226/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0473 - mae: 0.0323 - val_loss: 0.1351 - val_mae: 0.2160\n",
      "Epoch 227/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0475 - mae: 0.0322 - val_loss: 0.0092 - val_mae: 0.0857\n",
      "Epoch 228/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0473 - mae: 0.0323 - val_loss: 0.0089 - val_mae: 0.0819\n",
      "Epoch 229/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238/238 [==============================] - 103s 431ms/step - loss: -0.0473 - mae: 0.0322 - val_loss: 0.0605 - val_mae: 0.1343\n",
      "Epoch 230/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0474 - mae: 0.0322 - val_loss: 0.0695 - val_mae: 0.1519\n",
      "Epoch 231/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0476 - mae: 0.0321 - val_loss: 0.0668 - val_mae: 0.1429\n",
      "Epoch 232/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0482 - mae: 0.0319 - val_loss: 0.0255 - val_mae: 0.1027\n",
      "Epoch 233/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0478 - mae: 0.0320 - val_loss: 0.1185 - val_mae: 0.1897\n",
      "\n",
      "Epoch 00233: ReduceLROnPlateau reducing learning rate to 3.023308381671086e-05.\n",
      "Epoch 234/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0475 - mae: 0.0322 - val_loss: 0.0808 - val_mae: 0.1662\n",
      "Epoch 235/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0477 - mae: 0.0319 - val_loss: 0.0455 - val_mae: 0.1201\n",
      "Epoch 236/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0477 - mae: 0.0321 - val_loss: 0.0414 - val_mae: 0.1145\n",
      "Epoch 237/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0475 - mae: 0.0322 - val_loss: 0.0120 - val_mae: 0.0879\n",
      "Epoch 238/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0474 - mae: 0.0323 - val_loss: 0.0588 - val_mae: 0.1367\n",
      "Epoch 239/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0476 - mae: 0.0319 - val_loss: 0.0897 - val_mae: 0.1727\n",
      "Epoch 240/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0480 - mae: 0.0318 - val_loss: 0.0305 - val_mae: 0.1020\n",
      "Epoch 241/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0475 - mae: 0.0321 - val_loss: -0.0151 - val_mae: 0.0579\n",
      "Epoch 242/1000\n",
      "238/238 [==============================] - 102s 429ms/step - loss: -0.0477 - mae: 0.0321 - val_loss: 0.1361 - val_mae: 0.2174\n",
      "Epoch 243/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0475 - mae: 0.0321 - val_loss: 0.0530 - val_mae: 0.1344\n",
      "Epoch 244/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0477 - mae: 0.0318 - val_loss: 0.0234 - val_mae: 0.0999\n",
      "Epoch 245/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0477 - mae: 0.0321 - val_loss: 0.0215 - val_mae: 0.0940\n",
      "Epoch 246/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0476 - mae: 0.0320 - val_loss: 0.1079 - val_mae: 0.1815\n",
      "Epoch 247/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0477 - mae: 0.0319 - val_loss: -0.0031 - val_mae: 0.0694\n",
      "Epoch 248/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0475 - mae: 0.0322 - val_loss: 0.0086 - val_mae: 0.0819\n",
      "Epoch 249/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0480 - mae: 0.0320 - val_loss: 0.0639 - val_mae: 0.1367\n",
      "Epoch 250/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0477 - mae: 0.0321 - val_loss: 0.0244 - val_mae: 0.1003\n",
      "Epoch 251/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0477 - mae: 0.0319 - val_loss: 0.1417 - val_mae: 0.2324\n",
      "Epoch 252/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0476 - mae: 0.0320 - val_loss: 0.0257 - val_mae: 0.0989\n",
      "Epoch 253/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0478 - mae: 0.0319 - val_loss: 0.0884 - val_mae: 0.1602\n",
      "Epoch 254/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0476 - mae: 0.0321 - val_loss: -0.0010 - val_mae: 0.0712\n",
      "Epoch 255/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0479 - mae: 0.0318 - val_loss: 0.0570 - val_mae: 0.1282\n",
      "Epoch 256/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0477 - mae: 0.0320 - val_loss: 0.0257 - val_mae: 0.1009\n",
      "Epoch 257/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0476 - mae: 0.0320 - val_loss: 0.0284 - val_mae: 0.1012\n",
      "Epoch 258/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0477 - mae: 0.0320 - val_loss: 0.0611 - val_mae: 0.1435\n",
      "\n",
      "Epoch 00258: ReduceLROnPlateau reducing learning rate to 1.8139850726583973e-05.\n",
      "Epoch 259/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0477 - mae: 0.0318 - val_loss: 0.0222 - val_mae: 0.0969\n",
      "Epoch 260/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0475 - mae: 0.0321 - val_loss: -0.0125 - val_mae: 0.0608\n",
      "Epoch 261/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0477 - mae: 0.0319 - val_loss: 9.0774e-04 - val_mae: 0.0747\n",
      "Epoch 262/1000\n",
      "238/238 [==============================] - 103s 434ms/step - loss: -0.0478 - mae: 0.0320 - val_loss: 0.0576 - val_mae: 0.1405\n",
      "Epoch 263/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0479 - mae: 0.0316 - val_loss: 0.0287 - val_mae: 0.1071\n",
      "Epoch 264/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0475 - mae: 0.0321 - val_loss: 0.0181 - val_mae: 0.0915\n",
      "Epoch 265/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0479 - mae: 0.0318 - val_loss: -0.0195 - val_mae: 0.0536\n",
      "Epoch 266/1000\n",
      "238/238 [==============================] - 102s 430ms/step - loss: -0.0480 - mae: 0.0317 - val_loss: 0.0200 - val_mae: 0.0991\n",
      "Epoch 267/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0476 - mae: 0.0320 - val_loss: 0.0287 - val_mae: 0.1007\n",
      "Epoch 268/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0477 - mae: 0.0320 - val_loss: 0.0207 - val_mae: 0.0937\n",
      "Epoch 269/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0473 - mae: 0.0322 - val_loss: 0.0062 - val_mae: 0.0807\n",
      "Epoch 270/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0477 - mae: 0.0318 - val_loss: -0.0124 - val_mae: 0.0609\n",
      "Epoch 271/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0476 - mae: 0.0319 - val_loss: 0.0093 - val_mae: 0.0848\n",
      "Epoch 272/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0481 - mae: 0.0317 - val_loss: 0.0184 - val_mae: 0.0913\n",
      "Epoch 273/1000\n",
      "238/238 [==============================] - 103s 432ms/step - loss: -0.0475 - mae: 0.0322 - val_loss: -0.0027 - val_mae: 0.0694\n",
      "Epoch 274/1000\n",
      "238/238 [==============================] - 103s 434ms/step - loss: -0.0481 - mae: 0.0316 - val_loss: 0.0032 - val_mae: 0.0751\n",
      "Epoch 275/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0479 - mae: 0.0318 - val_loss: 0.0326 - val_mae: 0.1108\n",
      "Epoch 276/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0477 - mae: 0.0319 - val_loss: 0.0637 - val_mae: 0.1471\n",
      "Epoch 277/1000\n",
      "238/238 [==============================] - 103s 433ms/step - loss: -0.0478 - mae: 0.0321 - val_loss: 0.0296 - val_mae: 0.1044\n",
      "Epoch 278/1000\n",
      "238/238 [==============================] - 104s 435ms/step - loss: -0.0478 - mae: 0.0320 - val_loss: 0.0387 - val_mae: 0.1112\n",
      "Epoch 279/1000\n",
      "238/238 [==============================] - 104s 435ms/step - loss: -0.0480 - mae: 0.0318 - val_loss: 0.0495 - val_mae: 0.1232\n",
      "Epoch 280/1000\n",
      "238/238 [==============================] - 103s 434ms/step - loss: -0.0475 - mae: 0.0319 - val_loss: 0.0067 - val_mae: 0.0784\n",
      "Epoch 281/1000\n",
      "238/238 [==============================] - 103s 434ms/step - loss: -0.0480 - mae: 0.0317 - val_loss: 0.0143 - val_mae: 0.0864\n",
      "Epoch 282/1000\n",
      "238/238 [==============================] - 103s 435ms/step - loss: -0.0480 - mae: 0.0316 - val_loss: 0.0354 - val_mae: 0.1071\n",
      "\n",
      "Epoch 00282: ReduceLROnPlateau reducing learning rate to 1.0883909999392926e-05.\n",
      "Epoch 283/1000\n",
      "238/238 [==============================] - 103s 435ms/step - loss: -0.0479 - mae: 0.0319 - val_loss: -0.0149 - val_mae: 0.0580\n",
      "Epoch 284/1000\n",
      "238/238 [==============================] - 103s 434ms/step - loss: -0.0478 - mae: 0.0319 - val_loss: 0.0548 - val_mae: 0.1373\n",
      "Epoch 285/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 42/238 [====>.........................] - ETA: 1:19 - loss: -0.0483 - mae: 0.0316"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,BatchNormalization,Dense,Input,concatenate,Reshape,Flatten,AveragePooling2D,Conv2DTranspose\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "Generator = DoubleBase_Unet_modeling()\n",
    "optimizer = Adam(lr=0.003)\n",
    "modelpath = \"./datasets/models/pix2pix_mae_customloss_standard_8.h5\"\n",
    "cp = ModelCheckpoint(monitor = 'val_loss',filepath = modelpath,save_best_only=True)\n",
    "es = EarlyStopping(monitor = \"val_loss\",patience=80)\n",
    "reLR = ReduceLROnPlateau(monitor = 'val_loss',patience=17,factor=0.6,verbose=1)\n",
    "\n",
    "\n",
    "Generator.compile(loss = mae_standard,optimizer = optimizer,metrics=['mae'])\n",
    "Generator.fit(npy_x,npy_y,epochs=1000,validation_split=0.15,batch_size=64,callbacks=[cp,es,reLR])\n",
    "Generator.summary()\n",
    "\n",
    "\n",
    "'-0.5 분산오차 해바바'\n",
    "'0.0076'\n",
    "'0.0080'\n",
    "'0.002'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "475/475 [==============================] - 138s 283ms/step - loss: 0.1314 - mse: 0.1561 - val_loss: 1.7367 - val_mse: 1.7368\n",
      "Epoch 2/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: 0.0123 - mse: 0.0434 - val_loss: 1.7367 - val_mse: 1.7368\n",
      "Epoch 3/3000\n",
      "475/475 [==============================] - 124s 260ms/step - loss: -0.0050 - mse: 0.0267 - val_loss: 1.7368 - val_mse: 1.7368\n",
      "Epoch 4/3000\n",
      "475/475 [==============================] - 124s 262ms/step - loss: -0.0105 - mse: 0.0213 - val_loss: 1.7367 - val_mse: 1.7368\n",
      "Epoch 5/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0153 - mse: 0.0167 - val_loss: 1.7368 - val_mse: 1.7368\n",
      "Epoch 6/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0163 - mse: 0.0158 - val_loss: 1.7368 - val_mse: 1.7368\n",
      "Epoch 7/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0183 - mse: 0.0139 - val_loss: 1.7368 - val_mse: 1.7368\n",
      "Epoch 8/3000\n",
      "475/475 [==============================] - 124s 262ms/step - loss: -0.0178 - mse: 0.0144 - val_loss: 1.1971 - val_mse: 1.1992\n",
      "Epoch 9/3000\n",
      "475/475 [==============================] - 124s 260ms/step - loss: -0.0190 - mse: 0.0132 - val_loss: 1.7277 - val_mse: 1.7295\n",
      "Epoch 10/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0195 - mse: 0.0127 - val_loss: 1.3292 - val_mse: 1.3605\n",
      "Epoch 11/3000\n",
      "475/475 [==============================] - 124s 262ms/step - loss: -0.0206 - mse: 0.0117 - val_loss: 1.7093 - val_mse: 1.7178\n",
      "Epoch 12/3000\n",
      "475/475 [==============================] - 124s 262ms/step - loss: -0.0210 - mse: 0.0112 - val_loss: 1.3066 - val_mse: 1.3437\n",
      "Epoch 13/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0222 - mse: 0.0102 - val_loss: 1.6945 - val_mse: 1.7011\n",
      "Epoch 14/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0225 - mse: 0.0098 - val_loss: 0.4690 - val_mse: 0.5110\n",
      "Epoch 15/3000\n",
      "475/475 [==============================] - 124s 260ms/step - loss: -0.0226 - mse: 0.0096 - val_loss: 1.1993 - val_mse: 1.1995\n",
      "Epoch 16/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0230 - mse: 0.0093 - val_loss: 1.3220 - val_mse: 1.3634\n",
      "Epoch 17/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0234 - mse: 0.0090 - val_loss: 0.7544 - val_mse: 0.7872\n",
      "Epoch 18/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0242 - mse: 0.0082 - val_loss: 1.3373 - val_mse: 1.3833\n",
      "Epoch 19/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0242 - mse: 0.0082 - val_loss: 0.7093 - val_mse: 0.7502\n",
      "Epoch 20/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0243 - mse: 0.0080 - val_loss: 1.5920 - val_mse: 1.6208\n",
      "Epoch 21/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0247 - mse: 0.0076 - val_loss: 1.5268 - val_mse: 1.5523\n",
      "Epoch 22/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0248 - mse: 0.0075 - val_loss: 1.5184 - val_mse: 1.5441\n",
      "Epoch 23/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0245 - mse: 0.0079 - val_loss: 1.4680 - val_mse: 1.5058\n",
      "Epoch 24/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0246 - mse: 0.0077 - val_loss: 0.9034 - val_mse: 0.9351\n",
      "Epoch 25/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0253 - mse: 0.0071 - val_loss: 1.5879 - val_mse: 1.6200\n",
      "Epoch 26/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0255 - mse: 0.0069 - val_loss: 1.5313 - val_mse: 1.5607\n",
      "Epoch 27/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0254 - mse: 0.0070 - val_loss: 1.2771 - val_mse: 1.3256\n",
      "Epoch 28/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0256 - mse: 0.0068 - val_loss: 1.4300 - val_mse: 1.4543\n",
      "Epoch 29/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0258 - mse: 0.0066 - val_loss: 1.1946 - val_mse: 1.1971\n",
      "Epoch 30/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0249 - mse: 0.0076 - val_loss: 1.7367 - val_mse: 1.7368\n",
      "Epoch 31/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0261 - mse: 0.0065 - val_loss: 0.5757 - val_mse: 0.6161\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.001600000075995922.\n",
      "Epoch 32/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0261 - mse: 0.0062 - val_loss: 0.7482 - val_mse: 0.7882\n",
      "Epoch 33/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0265 - mse: 0.0059 - val_loss: 1.7311 - val_mse: 1.7324\n",
      "Epoch 34/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0264 - mse: 0.0060 - val_loss: 0.9712 - val_mse: 1.0040\n",
      "Epoch 35/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0264 - mse: 0.0060 - val_loss: 1.6706 - val_mse: 1.6873\n",
      "Epoch 36/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0266 - mse: 0.0058 - val_loss: 0.4820 - val_mse: 0.5253\n",
      "Epoch 37/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0268 - mse: 0.0057 - val_loss: 0.8877 - val_mse: 0.9321\n",
      "Epoch 38/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0267 - mse: 0.0057 - val_loss: 1.4869 - val_mse: 1.5065\n",
      "Epoch 39/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0268 - mse: 0.0057 - val_loss: 1.2596 - val_mse: 1.3070\n",
      "Epoch 40/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0269 - mse: 0.0055 - val_loss: 1.2738 - val_mse: 1.3001\n",
      "Epoch 41/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0271 - mse: 0.0053 - val_loss: 0.5647 - val_mse: 0.6039\n",
      "Epoch 42/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0270 - mse: 0.0053 - val_loss: 0.9552 - val_mse: 0.9981\n",
      "Epoch 43/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0272 - mse: 0.0053 - val_loss: 1.7359 - val_mse: 1.7367\n",
      "Epoch 44/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0272 - mse: 0.0053 - val_loss: 0.4957 - val_mse: 0.5383\n",
      "Epoch 45/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0272 - mse: 0.0052 - val_loss: 1.7287 - val_mse: 1.7312\n",
      "Epoch 46/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0273 - mse: 0.0051 - val_loss: 1.2311 - val_mse: 1.2554\n",
      "Epoch 47/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0272 - mse: 0.0053 - val_loss: 1.7058 - val_mse: 1.7109\n",
      "Epoch 48/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0274 - mse: 0.0050 - val_loss: 0.8341 - val_mse: 0.8791\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.0012800000607967378.\n",
      "Epoch 49/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0276 - mse: 0.0049 - val_loss: 0.7181 - val_mse: 0.7496\n",
      "Epoch 50/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0277 - mse: 0.0048 - val_loss: 0.3386 - val_mse: 0.3696\n",
      "Epoch 51/3000\n",
      "475/475 [==============================] - 123s 260ms/step - loss: -0.0276 - mse: 0.0049 - val_loss: 0.4878 - val_mse: 0.5284\n",
      "Epoch 52/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0278 - mse: 0.0047 - val_loss: 0.8669 - val_mse: 0.9015\n",
      "Epoch 53/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0276 - mse: 0.0048 - val_loss: 0.5504 - val_mse: 0.5931\n",
      "Epoch 54/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0276 - mse: 0.0049 - val_loss: 1.0163 - val_mse: 1.0358\n",
      "Epoch 55/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0280 - mse: 0.0046 - val_loss: 0.9133 - val_mse: 0.9561\n",
      "Epoch 56/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0279 - mse: 0.0046 - val_loss: 0.5021 - val_mse: 0.5425\n",
      "Epoch 57/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0279 - mse: 0.0046 - val_loss: 1.7299 - val_mse: 1.7325\n",
      "Epoch 58/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0279 - mse: 0.0046 - val_loss: 1.4202 - val_mse: 1.4468\n",
      "Epoch 59/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0279 - mse: 0.0045 - val_loss: 1.3034 - val_mse: 1.3205\n",
      "Epoch 60/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0278 - mse: 0.0045 - val_loss: 0.7239 - val_mse: 0.7497\n",
      "Epoch 61/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0280 - mse: 0.0045 - val_loss: 0.9159 - val_mse: 0.9519\n",
      "Epoch 62/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0281 - mse: 0.0044 - val_loss: 0.6251 - val_mse: 0.6703\n",
      "Epoch 63/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0280 - mse: 0.0044 - val_loss: 0.7146 - val_mse: 0.7597\n",
      "Epoch 64/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0281 - mse: 0.0044 - val_loss: 1.5112 - val_mse: 1.5238\n",
      "Epoch 65/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0280 - mse: 0.0043 - val_loss: 1.4757 - val_mse: 1.4952\n",
      "Epoch 66/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0280 - mse: 0.0044 - val_loss: 1.6797 - val_mse: 1.6885\n",
      "Epoch 67/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0282 - mse: 0.0043 - val_loss: 0.9190 - val_mse: 0.9435\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 0.0010240000672638416.\n",
      "Epoch 68/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0282 - mse: 0.0042 - val_loss: 0.6680 - val_mse: 0.7140\n",
      "Epoch 69/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0283 - mse: 0.0042 - val_loss: 0.3391 - val_mse: 0.3746\n",
      "Epoch 70/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0284 - mse: 0.0041 - val_loss: 0.7943 - val_mse: 0.8249\n",
      "Epoch 71/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0282 - mse: 0.0043 - val_loss: 1.4984 - val_mse: 1.5122\n",
      "Epoch 72/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0282 - mse: 0.0042 - val_loss: 0.3050 - val_mse: 0.3414\n",
      "Epoch 73/3000\n",
      "475/475 [==============================] - 124s 260ms/step - loss: -0.0284 - mse: 0.0041 - val_loss: 1.0475 - val_mse: 1.0826\n",
      "Epoch 74/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0284 - mse: 0.0041 - val_loss: 0.3281 - val_mse: 0.3612\n",
      "Epoch 75/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0282 - mse: 0.0042 - val_loss: 1.6963 - val_mse: 1.7034\n",
      "Epoch 76/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0284 - mse: 0.0041 - val_loss: 0.8558 - val_mse: 0.8952\n",
      "Epoch 77/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0284 - mse: 0.0040 - val_loss: 0.2050 - val_mse: 0.2429\n",
      "Epoch 78/3000\n",
      "475/475 [==============================] - 123s 260ms/step - loss: -0.0283 - mse: 0.0041 - val_loss: 0.2290 - val_mse: 0.2655\n",
      "Epoch 79/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0286 - mse: 0.0040 - val_loss: 1.1995 - val_mse: 1.1997\n",
      "Epoch 80/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0281 - mse: 0.0044 - val_loss: 0.6087 - val_mse: 0.6307\n",
      "Epoch 81/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0284 - mse: 0.0040 - val_loss: 0.2717 - val_mse: 0.3056\n",
      "Epoch 82/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0285 - mse: 0.0040 - val_loss: 1.1924 - val_mse: 1.2095\n",
      "Epoch 83/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0284 - mse: 0.0040 - val_loss: 0.2349 - val_mse: 0.2718\n",
      "Epoch 84/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0286 - mse: 0.0040 - val_loss: 0.5713 - val_mse: 0.6151\n",
      "Epoch 85/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0286 - mse: 0.0040 - val_loss: 0.9613 - val_mse: 0.9970\n",
      "Epoch 86/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0286 - mse: 0.0039 - val_loss: 1.4458 - val_mse: 1.4685\n",
      "Epoch 87/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0285 - mse: 0.0039 - val_loss: 0.4164 - val_mse: 0.4538\n",
      "Epoch 88/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0286 - mse: 0.0039 - val_loss: 0.2152 - val_mse: 0.2528\n",
      "Epoch 89/3000\n",
      "475/475 [==============================] - 125s 264ms/step - loss: -0.0286 - mse: 0.0039 - val_loss: 0.3989 - val_mse: 0.4388\n",
      "Epoch 90/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0285 - mse: 0.0039 - val_loss: 0.8015 - val_mse: 0.8226\n",
      "Epoch 91/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0287 - mse: 0.0039 - val_loss: 0.3385 - val_mse: 0.3802\n",
      "Epoch 92/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0286 - mse: 0.0039 - val_loss: 0.6362 - val_mse: 0.6670\n",
      "Epoch 93/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0286 - mse: 0.0038 - val_loss: 0.9537 - val_mse: 0.9876\n",
      "Epoch 94/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0287 - mse: 0.0039 - val_loss: 1.1394 - val_mse: 1.1497\n",
      "\n",
      "Epoch 00094: ReduceLROnPlateau reducing learning rate to 0.0008192000910639763.\n",
      "Epoch 95/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0287 - mse: 0.0038 - val_loss: 1.4939 - val_mse: 1.5138\n",
      "Epoch 96/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0287 - mse: 0.0038 - val_loss: 0.4583 - val_mse: 0.4960\n",
      "Epoch 97/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0286 - mse: 0.0038 - val_loss: 0.0720 - val_mse: 0.1027\n",
      "Epoch 98/3000\n",
      "475/475 [==============================] - 123s 260ms/step - loss: -0.0288 - mse: 0.0037 - val_loss: 1.0155 - val_mse: 1.0473\n",
      "Epoch 99/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0286 - mse: 0.0038 - val_loss: 0.3433 - val_mse: 0.3821\n",
      "Epoch 100/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0287 - mse: 0.0037 - val_loss: 0.5235 - val_mse: 0.5643\n",
      "Epoch 101/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0289 - mse: 0.0037 - val_loss: 0.5462 - val_mse: 0.5872\n",
      "Epoch 102/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0289 - mse: 0.0037 - val_loss: 0.5644 - val_mse: 0.6089\n",
      "Epoch 103/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0289 - mse: 0.0037 - val_loss: 0.7749 - val_mse: 0.8052\n",
      "Epoch 104/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0287 - mse: 0.0038 - val_loss: 0.6927 - val_mse: 0.7159\n",
      "Epoch 105/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0288 - mse: 0.0037 - val_loss: 0.6233 - val_mse: 0.6603\n",
      "Epoch 106/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0289 - mse: 0.0037 - val_loss: 0.6253 - val_mse: 0.6687\n",
      "Epoch 107/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0287 - mse: 0.0037 - val_loss: 0.8045 - val_mse: 0.8386\n",
      "Epoch 108/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0288 - mse: 0.0036 - val_loss: 0.6432 - val_mse: 0.6844\n",
      "Epoch 109/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0289 - mse: 0.0037 - val_loss: 0.7413 - val_mse: 0.7782\n",
      "Epoch 110/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0288 - mse: 0.0036 - val_loss: 0.5032 - val_mse: 0.5477\n",
      "Epoch 111/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0289 - mse: 0.0036 - val_loss: 0.3154 - val_mse: 0.3498\n",
      "Epoch 112/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0288 - mse: 0.0037 - val_loss: 0.3668 - val_mse: 0.4089\n",
      "Epoch 113/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0288 - mse: 0.0037 - val_loss: 0.6154 - val_mse: 0.6567\n",
      "Epoch 114/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0288 - mse: 0.0036 - val_loss: 0.6622 - val_mse: 0.6828\n",
      "\n",
      "Epoch 00114: ReduceLROnPlateau reducing learning rate to 0.0006553600542247295.\n",
      "Epoch 115/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0290 - mse: 0.0036 - val_loss: 0.5615 - val_mse: 0.6073\n",
      "Epoch 116/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0289 - mse: 0.0036 - val_loss: 0.2644 - val_mse: 0.3017\n",
      "Epoch 117/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0290 - mse: 0.0036 - val_loss: 0.2854 - val_mse: 0.3184\n",
      "Epoch 118/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0291 - mse: 0.0035 - val_loss: 0.2751 - val_mse: 0.3121\n",
      "Epoch 119/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0291 - mse: 0.0035 - val_loss: 1.4505 - val_mse: 1.4751\n",
      "Epoch 120/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0288 - mse: 0.0037 - val_loss: 0.0952 - val_mse: 0.1279\n",
      "Epoch 121/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0289 - mse: 0.0035 - val_loss: 0.6505 - val_mse: 0.6765\n",
      "Epoch 122/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0289 - mse: 0.0036 - val_loss: 0.4422 - val_mse: 0.4836\n",
      "Epoch 123/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0291 - mse: 0.0035 - val_loss: 0.7944 - val_mse: 0.8295\n",
      "Epoch 124/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0289 - mse: 0.0035 - val_loss: 0.4281 - val_mse: 0.4594\n",
      "Epoch 125/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0289 - mse: 0.0036 - val_loss: 0.1075 - val_mse: 0.1414\n",
      "Epoch 126/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0290 - mse: 0.0036 - val_loss: 1.3938 - val_mse: 1.4115\n",
      "Epoch 127/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0290 - mse: 0.0035 - val_loss: 0.0679 - val_mse: 0.0996\n",
      "Epoch 128/3000\n",
      "475/475 [==============================] - 124s 260ms/step - loss: -0.0291 - mse: 0.0035 - val_loss: 0.7206 - val_mse: 0.7560\n",
      "Epoch 129/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0289 - mse: 0.0035 - val_loss: 0.1145 - val_mse: 0.1460\n",
      "Epoch 130/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0290 - mse: 0.0035 - val_loss: 0.6998 - val_mse: 0.7404\n",
      "Epoch 131/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0290 - mse: 0.0035 - val_loss: 0.1416 - val_mse: 0.1758\n",
      "Epoch 132/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0290 - mse: 0.0035 - val_loss: 0.4914 - val_mse: 0.5326\n",
      "Epoch 133/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0290 - mse: 0.0035 - val_loss: 0.3965 - val_mse: 0.4372\n",
      "Epoch 134/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0291 - mse: 0.0034 - val_loss: 0.1689 - val_mse: 0.2027\n",
      "Epoch 135/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0290 - mse: 0.0035 - val_loss: 0.2181 - val_mse: 0.2526\n",
      "Epoch 136/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0290 - mse: 0.0035 - val_loss: 0.3234 - val_mse: 0.3620\n",
      "Epoch 137/3000\n",
      "475/475 [==============================] - 124s 262ms/step - loss: -0.0289 - mse: 0.0035 - val_loss: 1.1752 - val_mse: 1.1810\n",
      "Epoch 138/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0290 - mse: 0.0035 - val_loss: 0.3437 - val_mse: 0.3710\n",
      "Epoch 139/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0291 - mse: 0.0034 - val_loss: 0.4223 - val_mse: 0.4603\n",
      "Epoch 140/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0291 - mse: 0.0034 - val_loss: 0.4782 - val_mse: 0.5183\n",
      "Epoch 141/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0292 - mse: 0.0034 - val_loss: 0.6797 - val_mse: 0.7073\n",
      "Epoch 142/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0291 - mse: 0.0034 - val_loss: 1.4544 - val_mse: 1.4773\n",
      "Epoch 143/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0291 - mse: 0.0035 - val_loss: 0.5615 - val_mse: 0.5995\n",
      "Epoch 144/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0290 - mse: 0.0034 - val_loss: 0.4970 - val_mse: 0.5383\n",
      "\n",
      "Epoch 00144: ReduceLROnPlateau reducing learning rate to 0.0005242880433797836.\n",
      "Epoch 145/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0290 - mse: 0.0035 - val_loss: 0.1664 - val_mse: 0.1993\n",
      "Epoch 146/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0291 - mse: 0.0033 - val_loss: 0.6733 - val_mse: 0.7130\n",
      "Epoch 147/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0291 - mse: 0.0034 - val_loss: 0.5222 - val_mse: 0.5652\n",
      "Epoch 148/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0292 - mse: 0.0034 - val_loss: 0.1031 - val_mse: 0.1370\n",
      "Epoch 149/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0291 - mse: 0.0034 - val_loss: 0.3270 - val_mse: 0.3670\n",
      "Epoch 150/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0291 - mse: 0.0034 - val_loss: 0.4932 - val_mse: 0.5333\n",
      "Epoch 151/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0290 - mse: 0.0034 - val_loss: 0.2581 - val_mse: 0.2909\n",
      "Epoch 152/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0292 - mse: 0.0033 - val_loss: 0.1046 - val_mse: 0.1382\n",
      "Epoch 153/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0292 - mse: 0.0033 - val_loss: 0.4078 - val_mse: 0.4489\n",
      "Epoch 154/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0292 - mse: 0.0033 - val_loss: 0.4947 - val_mse: 0.5250\n",
      "Epoch 155/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0292 - mse: 0.0034 - val_loss: 0.9897 - val_mse: 1.0115\n",
      "Epoch 156/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0292 - mse: 0.0033 - val_loss: 0.4543 - val_mse: 0.4950\n",
      "Epoch 157/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0292 - mse: 0.0033 - val_loss: 0.0671 - val_mse: 0.0972\n",
      "Epoch 158/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0291 - mse: 0.0033 - val_loss: 0.1325 - val_mse: 0.1643\n",
      "Epoch 159/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0292 - mse: 0.0033 - val_loss: 0.0642 - val_mse: 0.0970\n",
      "Epoch 160/3000\n",
      "475/475 [==============================] - 124s 260ms/step - loss: -0.0293 - mse: 0.0033 - val_loss: 0.0777 - val_mse: 0.1082\n",
      "Epoch 161/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0291 - mse: 0.0033 - val_loss: 0.7321 - val_mse: 0.7710\n",
      "Epoch 162/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0291 - mse: 0.0034 - val_loss: 0.2215 - val_mse: 0.2557\n",
      "Epoch 163/3000\n",
      "475/475 [==============================] - 124s 262ms/step - loss: -0.0291 - mse: 0.0033 - val_loss: 0.9395 - val_mse: 0.9722\n",
      "Epoch 164/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0293 - mse: 0.0033 - val_loss: 0.1606 - val_mse: 0.1925\n",
      "Epoch 165/3000\n",
      "475/475 [==============================] - 126s 264ms/step - loss: -0.0292 - mse: 0.0033 - val_loss: 0.7630 - val_mse: 0.8061\n",
      "Epoch 166/3000\n",
      "475/475 [==============================] - 125s 263ms/step - loss: -0.0292 - mse: 0.0033 - val_loss: 0.7864 - val_mse: 0.8208\n",
      "Epoch 167/3000\n",
      "475/475 [==============================] - 125s 263ms/step - loss: -0.0293 - mse: 0.0033 - val_loss: 0.1841 - val_mse: 0.2213\n",
      "Epoch 168/3000\n",
      "475/475 [==============================] - 125s 263ms/step - loss: -0.0293 - mse: 0.0033 - val_loss: 0.0576 - val_mse: 0.0853\n",
      "Epoch 169/3000\n",
      "475/475 [==============================] - 124s 262ms/step - loss: -0.0292 - mse: 0.0033 - val_loss: 0.5103 - val_mse: 0.5526\n",
      "Epoch 170/3000\n",
      "475/475 [==============================] - 125s 263ms/step - loss: -0.0292 - mse: 0.0033 - val_loss: 0.6767 - val_mse: 0.7058\n",
      "Epoch 171/3000\n",
      "475/475 [==============================] - 125s 262ms/step - loss: -0.0293 - mse: 0.0032 - val_loss: 0.1062 - val_mse: 0.1386\n",
      "Epoch 172/3000\n",
      "475/475 [==============================] - 124s 262ms/step - loss: -0.0293 - mse: 0.0032 - val_loss: 0.1680 - val_mse: 0.2008\n",
      "Epoch 173/3000\n",
      "475/475 [==============================] - 124s 262ms/step - loss: -0.0292 - mse: 0.0033 - val_loss: 0.7446 - val_mse: 0.7838\n",
      "Epoch 174/3000\n",
      "475/475 [==============================] - 124s 262ms/step - loss: -0.0292 - mse: 0.0033 - val_loss: 0.5132 - val_mse: 0.5587\n",
      "Epoch 175/3000\n",
      "475/475 [==============================] - 124s 262ms/step - loss: -0.0292 - mse: 0.0033 - val_loss: 1.0719 - val_mse: 1.0953\n",
      "Epoch 176/3000\n",
      "475/475 [==============================] - 124s 262ms/step - loss: -0.0289 - mse: 0.0035 - val_loss: 0.3615 - val_mse: 0.3967\n",
      "Epoch 177/3000\n",
      "475/475 [==============================] - 124s 262ms/step - loss: -0.0293 - mse: 0.0033 - val_loss: 0.2774 - val_mse: 0.3150\n",
      "Epoch 178/3000\n",
      "475/475 [==============================] - 124s 262ms/step - loss: -0.0292 - mse: 0.0032 - val_loss: 0.1790 - val_mse: 0.2128\n",
      "Epoch 179/3000\n",
      "475/475 [==============================] - 124s 262ms/step - loss: -0.0292 - mse: 0.0033 - val_loss: 0.3636 - val_mse: 0.3917\n",
      "Epoch 180/3000\n",
      "475/475 [==============================] - 124s 262ms/step - loss: -0.0291 - mse: 0.0033 - val_loss: 0.3654 - val_mse: 0.3979\n",
      "Epoch 181/3000\n",
      "475/475 [==============================] - 124s 262ms/step - loss: -0.0293 - mse: 0.0033 - val_loss: 0.0640 - val_mse: 0.0965\n",
      "Epoch 182/3000\n",
      "475/475 [==============================] - 124s 262ms/step - loss: -0.0293 - mse: 0.0032 - val_loss: 1.1296 - val_mse: 1.1396\n",
      "Epoch 183/3000\n",
      "475/475 [==============================] - 124s 262ms/step - loss: -0.0292 - mse: 0.0033 - val_loss: 0.2046 - val_mse: 0.2417\n",
      "Epoch 184/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0292 - mse: 0.0033 - val_loss: 0.5106 - val_mse: 0.5418\n",
      "Epoch 185/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0292 - mse: 0.0032 - val_loss: 0.0308 - val_mse: 0.0610\n",
      "Epoch 186/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0292 - mse: 0.0032 - val_loss: 0.4397 - val_mse: 0.4768\n",
      "Epoch 187/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0293 - mse: 0.0032 - val_loss: 0.0956 - val_mse: 0.1282\n",
      "Epoch 188/3000\n",
      "475/475 [==============================] - 124s 262ms/step - loss: -0.0294 - mse: 0.0032 - val_loss: 0.4758 - val_mse: 0.5071\n",
      "Epoch 189/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0293 - mse: 0.0032 - val_loss: 0.3501 - val_mse: 0.3779\n",
      "Epoch 190/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0292 - mse: 0.0032 - val_loss: 0.4597 - val_mse: 0.4855\n",
      "Epoch 191/3000\n",
      "475/475 [==============================] - 125s 262ms/step - loss: -0.0293 - mse: 0.0032 - val_loss: 0.9525 - val_mse: 0.9842\n",
      "Epoch 192/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0293 - mse: 0.0032 - val_loss: 0.1695 - val_mse: 0.2072\n",
      "Epoch 193/3000\n",
      "475/475 [==============================] - 124s 262ms/step - loss: -0.0292 - mse: 0.0032 - val_loss: 0.3785 - val_mse: 0.4204\n",
      "Epoch 194/3000\n",
      "475/475 [==============================] - 124s 262ms/step - loss: -0.0293 - mse: 0.0032 - val_loss: 0.2746 - val_mse: 0.3138\n",
      "Epoch 195/3000\n",
      "475/475 [==============================] - 124s 262ms/step - loss: -0.0293 - mse: 0.0032 - val_loss: 0.8204 - val_mse: 0.8583\n",
      "Epoch 196/3000\n",
      "475/475 [==============================] - 124s 262ms/step - loss: -0.0292 - mse: 0.0032 - val_loss: 0.4531 - val_mse: 0.4763\n",
      "Epoch 197/3000\n",
      "475/475 [==============================] - 124s 262ms/step - loss: -0.0292 - mse: 0.0032 - val_loss: 0.1259 - val_mse: 0.1568\n",
      "Epoch 198/3000\n",
      "475/475 [==============================] - 124s 262ms/step - loss: -0.0293 - mse: 0.0032 - val_loss: 0.1875 - val_mse: 0.2182\n",
      "Epoch 199/3000\n",
      "475/475 [==============================] - 125s 262ms/step - loss: -0.0293 - mse: 0.0032 - val_loss: 0.5276 - val_mse: 0.5731\n",
      "Epoch 200/3000\n",
      "475/475 [==============================] - 125s 263ms/step - loss: -0.0293 - mse: 0.0032 - val_loss: 0.3786 - val_mse: 0.4227\n",
      "Epoch 201/3000\n",
      "475/475 [==============================] - 125s 263ms/step - loss: -0.0293 - mse: 0.0032 - val_loss: 0.2874 - val_mse: 0.3281\n",
      "Epoch 202/3000\n",
      "475/475 [==============================] - 125s 262ms/step - loss: -0.0292 - mse: 0.0032 - val_loss: 0.4264 - val_mse: 0.4615\n",
      "\n",
      "Epoch 00202: ReduceLROnPlateau reducing learning rate to 0.0004194304347038269.\n",
      "Epoch 203/3000\n",
      "475/475 [==============================] - 125s 262ms/step - loss: -0.0293 - mse: 0.0032 - val_loss: 0.0165 - val_mse: 0.0458\n",
      "Epoch 204/3000\n",
      "475/475 [==============================] - 124s 261ms/step - loss: -0.0294 - mse: 0.0031 - val_loss: 0.7923 - val_mse: 0.8280\n",
      "Epoch 205/3000\n",
      "475/475 [==============================] - 125s 262ms/step - loss: -0.0293 - mse: 0.0032 - val_loss: 0.1592 - val_mse: 0.1957\n",
      "Epoch 206/3000\n",
      "475/475 [==============================] - 125s 263ms/step - loss: -0.0294 - mse: 0.0031 - val_loss: 0.6410 - val_mse: 0.6852\n",
      "Epoch 207/3000\n",
      "475/475 [==============================] - 124s 262ms/step - loss: -0.0293 - mse: 0.0032 - val_loss: 0.1084 - val_mse: 0.1342\n",
      "Epoch 208/3000\n",
      "475/475 [==============================] - 124s 262ms/step - loss: -0.0294 - mse: 0.0032 - val_loss: 0.6430 - val_mse: 0.6801\n",
      "Epoch 209/3000\n",
      "475/475 [==============================] - 125s 262ms/step - loss: -0.0293 - mse: 0.0032 - val_loss: 0.2236 - val_mse: 0.2628\n",
      "Epoch 210/3000\n",
      "475/475 [==============================] - 125s 263ms/step - loss: -0.0293 - mse: 0.0032 - val_loss: 0.3309 - val_mse: 0.3725\n",
      "Epoch 211/3000\n",
      "475/475 [==============================] - 125s 263ms/step - loss: -0.0294 - mse: 0.0031 - val_loss: 0.3825 - val_mse: 0.4205\n",
      "Epoch 212/3000\n",
      "475/475 [==============================] - 125s 264ms/step - loss: -0.0294 - mse: 0.0031 - val_loss: 0.2859 - val_mse: 0.3282\n",
      "Epoch 213/3000\n",
      "475/475 [==============================] - 125s 264ms/step - loss: -0.0295 - mse: 0.0031 - val_loss: 0.3566 - val_mse: 0.3911\n",
      "Epoch 214/3000\n",
      "475/475 [==============================] - 125s 263ms/step - loss: -0.0293 - mse: 0.0032 - val_loss: 0.2225 - val_mse: 0.2517\n",
      "Epoch 215/3000\n",
      "475/475 [==============================] - 125s 263ms/step - loss: -0.0295 - mse: 0.0031 - val_loss: 0.3189 - val_mse: 0.3434\n",
      "Epoch 216/3000\n",
      "475/475 [==============================] - 125s 263ms/step - loss: -0.0293 - mse: 0.0032 - val_loss: 0.3374 - val_mse: 0.3721\n",
      "Epoch 217/3000\n",
      "475/475 [==============================] - 125s 263ms/step - loss: -0.0294 - mse: 0.0031 - val_loss: 0.6111 - val_mse: 0.6525\n",
      "Epoch 218/3000\n",
      "310/475 [==================>...........] - ETA: 40s - loss: -0.0294 - mse: 0.0031"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a28a2b2f13d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmse_standard\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mGenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnpy_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnpy_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreLR\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,BatchNormalization,Dense,Input,concatenate,Reshape,Flatten,AveragePooling2D,Conv2DTranspose\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "Generator = DoubleBase_Unet_modeling()\n",
    "optimizer = Adam(lr=0.002)\n",
    "modelpath = \"./datasets/models/pix2pix_mse_customloss_standard_20.h5\"\n",
    "cp = ModelCheckpoint(monitor = 'val_loss',filepath = modelpath,save_best_only=True)\n",
    "es = EarlyStopping(monitor = \"val_loss\",patience=150)\n",
    "reLR = ReduceLROnPlateau(monitor = 'val_loss',patience=21,factor=0.8,verbose=1)\n",
    "\n",
    "\n",
    "Generator.compile(loss = mse_standard,optimizer = optimizer,metrics=['mse'])\n",
    "Generator.fit(npy_x,npy_y,epochs=3000,validation_split=0.15,batch_size=32,callbacks=[cp,es,reLR])\n",
    "Generator.summary()\n",
    "\n",
    "'mse로 했었어 그래서 mse+분산으로할거야'\n",
    "'0.0165'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,BatchNormalization,Dense,Input,concatenate,Reshape,Flatten,AveragePooling2D,Conv2DTranspose\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "optimizer = Adam(lr=0.003)\n",
    "modelpath = \"./datasets/models/pix2pix_mae_color.h5\"\n",
    "cp = ModelCheckpoint(monitor = 'val_loss',filepath = modelpath,save_best_only=True)\n",
    "es = EarlyStopping(monitor = \"val_loss\",patience=50)\n",
    "reLR = ReduceLROnPlateau(monitor = 'val_loss',patience=13,factor=0.6)\n",
    "\n",
    "\n",
    "Generator.compile(loss = 'mae',optimizer = optimizer)\n",
    "Generator.fit(npy_x,npy_y,epochs=1000,validation_split=0.15,batch_size=32,callbacks=[cp,es,reLR])\n",
    "Generator.summary()\n",
    "\n",
    "\n",
    "'868'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,BatchNormalization,Dense,Input,concatenate,Reshape,Flatten,AveragePooling2D,Conv2DTranspose\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "optimizer = Adam(lr=0.003)\n",
    "modelpath = \"./datasets/models/pix2pix_mse_mae_color.h5\"\n",
    "cp = ModelCheckpoint(monitor = 'val_loss',filepath = modelpath,save_best_only=True)\n",
    "es = EarlyStopping(monitor = \"val_loss\",patience=50)\n",
    "reLR = ReduceLROnPlateau(monitor = 'val_loss',patience=13,factor=0.6)\n",
    "\n",
    "\n",
    "Generator.compile(loss = mae_and_mse,optimizer = optimizer)\n",
    "Generator.fit(npy_x,npy_y,epochs=100,validation_split=0.15,batch_size=32,callbacks=[cp,es,reLR])\n",
    "Generator.summary()\n",
    "\n",
    "\n",
    "'868'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_and_mse(y_true,y_pred):\n",
    "    return tf.math.reduce_mean(tf.square(y_true-y_pred)) + *tf.math.reduce_mean(tf.abs(y_true-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,BatchNormalization,Dense,Input,concatenate,Reshape,Flatten,AveragePooling2D,Conv2DTranspose\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "optimizer = Adam(lr=0.003)\n",
    "modelpath = \"./datasets/models/pix2pix_mse_2mae.h5\"\n",
    "cp = ModelCheckpoint(monitor = 'val_loss',filepath = modelpath,save_best_only=True)\n",
    "es = EarlyStopping(monitor = \"val_loss\",patience=50)\n",
    "reLR = ReduceLROnPlateau(monitor = 'val_loss',patience=13,factor=0.6)\n",
    "\n",
    "\n",
    "Generator.compile(loss = mae_and_mse,optimizer = optimizer)\n",
    "Generator.fit(npy_x,npy_y,epochs=100,validation_split=0.15,batch_size=32,callbacks=[cp,es,reLR])\n",
    "Generator.summary()\n",
    "\n",
    "\n",
    "'868'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_and_mse5(y_true,y_pred):\n",
    "    return tf.math.reduce_mean(tf.square(y_true-y_pred)) + 25*tf.math.reduce_mean(tf.abs(y_true-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,BatchNormalization,Dense,Input,concatenate,Reshape,Flatten,AveragePooling2D,Conv2DTranspose\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "optimizer = Adam(lr=0.003)\n",
    "modelpath = \"./datasets/models/pix2pix_mse_25mae.h5\"\n",
    "cp = ModelCheckpoint(monitor = 'val_loss',filepath = modelpath,save_best_only=True)\n",
    "es = EarlyStopping(monitor = \"val_loss\",patience=50)\n",
    "reLR = ReduceLROnPlateau(monitor = 'val_loss',patience=13,factor=0.6)\n",
    "\n",
    "\n",
    "Generator.compile(loss = mae_and_mse5,optimizer = optimizer)\n",
    "Generator.fit(npy_x,npy_y,epochs=100,validation_split=0.15,batch_size=32,callbacks=[cp,es,reLR])\n",
    "Generator.summary()\n",
    "\n",
    "\n",
    "'868'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,BatchNormalization,Dense,Input,concatenate,Reshape,Flatten,AveragePooling2D,Conv2DTranspose\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "Generator = DoubleBase_Unet_modeling()\n",
    "optimizer = Adam(lr=0.003)\n",
    "modelpath = \"C:/models/double_model.h5\"\n",
    "cp = ModelCheckpoint(monitor = 'val_loss',filepath = modelpath,save_best_only=True)\n",
    "es = EarlyStopping(monitor = \"val_loss\",patience=50)\n",
    "reLR = ReduceLROnPlateau(monitor = 'val_loss',patience=13,factor=0.6)\n",
    "\n",
    "Generator = DoubleBase_Unet_modeling()\n",
    "Generator.compile(loss = pix2pix_loss,optimizer = optimizer)\n",
    "Generator.fit(npy_x,npy_y,epochs=1000,validation_split=0.15,batch_size=16,callbacks=[cp,es,reLR])\n",
    "Generator.summary()\n",
    "\n",
    "\n",
    "'868'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,BatchNormalization,Dense,Input,concatenate,Reshape,Flatten,AveragePooling2D,Conv2DTranspose\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "optimizer = Adam(lr=0.003)\n",
    "modelpath = \"C:/models/double_model_double.h5\"\n",
    "cp = ModelCheckpoint(monitor = 'val_loss',filepath = modelpath,save_best_only=True)\n",
    "es = EarlyStopping(monitor = \"val_loss\",patience=50)\n",
    "reLR = ReduceLROnPlateau(monitor = 'val_loss',patience=13,factor=0.6)\n",
    "\n",
    "Generator = ReverseUnet_modeling()\n",
    "Generator.compile(loss = pix2pix_loss,optimizer = optimizer)\n",
    "Generator.fit(npy_x,npy_y,epochs=1000,validation_split=0.15,batch_size=16,callbacks=[cp,es,reLR])\n",
    "Generator.summary()\n",
    "\n",
    "\n",
    "'868'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Conv2D,BatchNormalization,Dropout,Activation,LeakyReLU,UpSampling2D,Input,Dense,Reshape,Flatten,Conv2DTranspose,ReLU,concatenate,ZeroPadding2D\n",
    "import numpy as np\n",
    "\n",
    "def DoubleBase_Unet_modeling(kernel_size=4,drop_out=0.5):\n",
    "    initializer = tf.random_normal_initializer(0.,0.02)\n",
    "    inputs = Input(shape=(256,256,3))\n",
    "    layer1 = Conv2D(filters=128,kernel_size=kernel_size,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(inputs)\n",
    "    layer1 = LeakyReLU()(layer1)\n",
    "    layer1_ = layer1\n",
    "\n",
    "    layer2 = Conv2D(filters=256,kernel_size=kernel_size,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(layer1)\n",
    "    layer2_ = BatchNormalization()(layer2)\n",
    "    layer2 = LeakyReLU()(layer2_)\n",
    "\n",
    "    layer3 = Conv2D(filters=512,kernel_size=kernel_size,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(layer2)\n",
    "    layer3_ = BatchNormalization()(layer3)\n",
    "    layer3 = LeakyReLU()(layer3_)\n",
    "\n",
    "    layer4 = Conv2D(filters=1024,kernel_size=kernel_size,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(layer3)\n",
    "    layer4_ = BatchNormalization()(layer4)\n",
    "    layer4 = LeakyReLU()(layer4_)\n",
    "\n",
    "    layer5 = Conv2D(filters=1024,kernel_size=kernel_size,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(layer4)\n",
    "    layer5_ = BatchNormalization()(layer5)\n",
    "    layer5 = LeakyReLU()(layer5_)\n",
    "\n",
    "    layer6 = Conv2D(filters=1024,kernel_size=kernel_size,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(layer5)\n",
    "    layer6_ = BatchNormalization()(layer6)\n",
    "    layer6 = LeakyReLU()(layer6_)\n",
    "\n",
    "    layer7 = Conv2D(filters=1024,kernel_size=kernel_size,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(layer6)\n",
    "    layer7_ = BatchNormalization()(layer7)\n",
    "    layer7 = LeakyReLU()(layer7_)\n",
    "\n",
    "\n",
    "\n",
    "    layer8 = Conv2D(filters=1024,kernel_size=kernel_size,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(layer7)\n",
    "    layer8_ = BatchNormalization()(layer8)\n",
    "    layer8 = LeakyReLU()(layer8_)\n",
    "\n",
    "\n",
    "\n",
    "    layer9 = Conv2DTranspose(filters=1024,kernel_size=kernel_size,strides=2,padding='same',kernel_initializer=initializer,use_bias=False)(layer8)\n",
    "    layer9 = BatchNormalization()(layer9)\n",
    "    layer9 = layer9+layer7_\n",
    "    layer9 = Dropout(drop_out)(layer9)\n",
    "    layer9 = ReLU()(layer9)\n",
    "\n",
    "    layer10 = Conv2DTranspose(filters=1024,kernel_size=kernel_size,strides=2,padding='same',kernel_initializer=initializer,use_bias=False)(layer9)\n",
    "    layer10 = BatchNormalization()(layer10)\n",
    "    layer10 = concatenate([layer10,layer6_])\n",
    "    layer10 = Dropout(drop_out)(layer10)\n",
    "    layer10 = ReLU()(layer10)\n",
    "\n",
    "    layer11 = Conv2DTranspose(filters=1024,kernel_size=kernel_size,strides=2,padding='same',kernel_initializer=initializer,use_bias=False)(layer10)\n",
    "    layer11 = BatchNormalization()(layer11)\n",
    "    layer11 = layer11+layer5_\n",
    "    layer11 = Dropout(drop_out)(layer11)\n",
    "    layer11 = ReLU()(layer11)\n",
    "\n",
    "    layer12 = Conv2DTranspose(filters=1024,kernel_size=kernel_size,strides=2,padding='same',kernel_initializer=initializer,use_bias=False)(layer11)\n",
    "    layer12 = BatchNormalization()(layer12)\n",
    "    layer12 = layer12+layer4_\n",
    "    layer12 = ReLU()(layer12)\n",
    "\n",
    "    layer13 = Conv2DTranspose(filters=512,kernel_size=kernel_size,strides=2,padding='same',kernel_initializer=initializer,use_bias=False)(layer12)\n",
    "    layer13 = BatchNormalization()(layer13)\n",
    "    layer13 = layer13+layer3_\n",
    "    layer13 = ReLU()(layer13)\n",
    "\n",
    "    layer14 = Conv2DTranspose(filters=256,kernel_size=kernel_size,strides=2,padding='same',kernel_initializer=initializer,use_bias=False)(layer13)\n",
    "    layer14 = BatchNormalization()(layer14)\n",
    "    layer14 = layer14+layer2_\n",
    "    layer14 = ReLU()(layer14)\n",
    "\n",
    "    layer15 = Conv2DTranspose(filters=128,kernel_size=kernel_size,strides=2,padding='same',kernel_initializer=initializer,use_bias=False)(layer14)\n",
    "    layer15 = BatchNormalization()(layer15)\n",
    "    layer15 = layer15+layer1_\n",
    "    layer15 = ReLU()(layer15)\n",
    "\n",
    "    outputs = Conv2DTranspose(3,4,strides=2,padding='same',kernel_initializer=initializer,activation='tanh')(layer15)\n",
    "\n",
    "    Generator = Model(inputs=inputs,outputs=outputs)\n",
    "    return Generator\n",
    "\n",
    "#model = DoubleBase_Unet_modeling()\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Conv2D,BatchNormalization,Dropout,Activation,LeakyReLU,UpSampling2D,Input,Dense,Reshape,Flatten,Conv2DTranspose,ReLU,concatenate,ZeroPadding2D\n",
    "import numpy as np\n",
    "\n",
    "def Reverse_Unet_down_modeling(kernel_size=4,drop_out=0.5):\n",
    "    initializer = tf.random_normal_initializer(0.,0.02)\n",
    "    inputs = Input(shape=(256,256,3))\n",
    "    layer1 = Conv2DTranspose(filters=512,kernel_size=kernel_size,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(inputs)\n",
    "    layer1 = LeakyReLU()(layer1)\n",
    "    layer1_ = layer1\n",
    "\n",
    "    layer2 = Conv2DTranspose(filters=256,kernel_size=kernel_size,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(layer1)\n",
    "    layer2_ = BatchNormalization()(layer2)\n",
    "    layer2 = LeakyReLU()(layer2_)\n",
    "\n",
    "    layer3 = Conv2DTranspose(filters=128,kernel_size=kernel_size,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(layer2)\n",
    "    layer3_ = BatchNormalization()(layer3)\n",
    "    layer3 = LeakyReLU()(layer3_)\n",
    "\n",
    "    layer4 = Conv2DTranspose(filters=64,kernel_size=kernel_size,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(layer3)\n",
    "    layer4_ = BatchNormalization()(layer4)\n",
    "    layer4 = LeakyReLU()(layer4_)\n",
    "\n",
    "    layer5 = Conv2DTranspose(filters=64,kernel_size=kernel_size,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(layer4)\n",
    "    layer5_ = BatchNormalization()(layer5)\n",
    "    layer5 = LeakyReLU()(layer5_)\n",
    "\n",
    "    layer6 = Conv2DTranspose(filters=64,kernel_size=kernel_size,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(layer5)\n",
    "    layer6_ = BatchNormalization()(layer6)\n",
    "    layer6 = LeakyReLU()(layer6_)\n",
    "\n",
    "    layer7 = Conv2DTranspose(filters=64,kernel_size=kernel_size,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(layer6)\n",
    "    layer7_ = BatchNormalization()(layer7)\n",
    "    layer7 = LeakyReLU()(layer7_)\n",
    "\n",
    "\n",
    "\n",
    "    layer8 = Conv2DTranspose(filters=64,kernel_size=kernel_size,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(layer7)\n",
    "    layer8_ = BatchNormalization()(layer8)\n",
    "    layer8 = LeakyReLU()(layer8_)\n",
    "\n",
    "\n",
    "\n",
    "    layer9 = Conv2D(filters=64,kernel_size=kernel_size,strides=2,padding='same',kernel_initializer=initializer,use_bias=False)(layer8)\n",
    "    layer9 = BatchNormalization()(layer9)\n",
    "    layer9 = layer9+layer7_\n",
    "    layer9 = Dropout(drop_out)(layer9)\n",
    "    layer9 = ReLU()(layer9)\n",
    "\n",
    "    layer10 = Conv2D(filters=64,kernel_size=kernel_size,strides=2,padding='same',kernel_initializer=initializer,use_bias=False)(layer9)\n",
    "    layer10 = BatchNormalization()(layer10)\n",
    "    layer10 = concatenate([layer10,layer6_])\n",
    "    layer10 = Dropout(drop_out)(layer10)\n",
    "    layer10 = ReLU()(layer10)\n",
    "\n",
    "    layer11 = Conv2D(filters=64,kernel_size=kernel_size,strides=2,padding='same',kernel_initializer=initializer,use_bias=False)(layer10)\n",
    "    layer11 = BatchNormalization()(layer11)\n",
    "    layer11 = layer11+layer5_\n",
    "    layer11 = Dropout(drop_out)(layer11)\n",
    "    layer11 = ReLU()(layer11)\n",
    "\n",
    "    layer12 = Conv2D(filters=64,kernel_size=kernel_size,strides=2,padding='same',kernel_initializer=initializer,use_bias=False)(layer11)\n",
    "    layer12 = BatchNormalization()(layer12)\n",
    "    layer12 = layer12+layer4_\n",
    "    layer12 = ReLU()(layer12)\n",
    "\n",
    "    layer13 = Conv2D(filters=128,kernel_size=kernel_size,strides=2,padding='same',kernel_initializer=initializer,use_bias=False)(layer12)\n",
    "    layer13 = BatchNormalization()(layer13)\n",
    "    layer13 = layer13+layer3_\n",
    "    layer13 = ReLU()(layer13)\n",
    "\n",
    "    layer14 = Conv2D(filters=256,kernel_size=kernel_size,strides=2,padding='same',kernel_initializer=initializer,use_bias=False)(layer13)\n",
    "    layer14 = BatchNormalization()(layer14)\n",
    "    layer14 = layer14+layer2_\n",
    "    layer14 = ReLU()(layer14)\n",
    "\n",
    "    layer15 = Conv2D(filters=512,kernel_size=kernel_size,strides=2,padding='same',kernel_initializer=initializer,use_bias=False)(layer14)\n",
    "    layer15 = BatchNormalization()(layer15)\n",
    "    layer15 = layer15+layer1_\n",
    "    layer15 = ReLU()(layer15)\n",
    "\n",
    "    outputs = Conv2D(3,4,strides=2,padding='same',kernel_initializer=initializer,activation='tanh')(layer15)\n",
    "\n",
    "    Generator = Model(inputs=inputs,outputs=outputs)\n",
    "    return Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Conv2D,BatchNormalization,Dropout,Activation,LeakyReLU,UpSampling2D,Input,Dense,Reshape,Flatten,Conv2DTranspose,ReLU,concatenate,ZeroPadding2D\n",
    "import numpy as np\n",
    "\n",
    "def ReverseUnet_modeling(kernel_size=4,drop_out=0.5):\n",
    "    initializer = tf.random_normal_initializer(0.,0.02)\n",
    "    inputs = Input(shape=(256,256,3))\n",
    "    layer1 = Conv2DTranspose(filters=64,kernel_size=kernel_size,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(inputs)\n",
    "    layer1 = LeakyReLU()(layer1)\n",
    "    layer1_ = layer1\n",
    "\n",
    "    layer2 = Conv2DTranspose(filters=128,kernel_size=kernel_size,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(layer1)\n",
    "    layer2_ = BatchNormalization()(layer2)\n",
    "    layer2 = LeakyReLU()(layer2_)\n",
    "\n",
    "    layer3 = Conv2DTranspose(filters=256,kernel_size=kernel_size,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(layer2)\n",
    "    layer3_ = BatchNormalization()(layer3)\n",
    "    layer3 = LeakyReLU()(layer3_)\n",
    "\n",
    "    layer4 = Conv2DTranspose(filters=512,kernel_size=kernel_size,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(layer3)\n",
    "    layer4_ = BatchNormalization()(layer4)\n",
    "    layer4 = LeakyReLU()(layer4_)\n",
    "\n",
    "    layer5 = Conv2DTranspose(filters=512,kernel_size=kernel_size,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(layer4)\n",
    "    layer5_ = BatchNormalization()(layer5)\n",
    "    layer5 = LeakyReLU()(layer5_)\n",
    "\n",
    "    layer6 = Conv2DTranspose(filters=512,kernel_size=kernel_size,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(layer5)\n",
    "    layer6_ = BatchNormalization()(layer6)\n",
    "    layer6 = LeakyReLU()(layer6_)\n",
    "\n",
    "    layer7 = Conv2DTranspose(filters=512,kernel_size=kernel_size,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(layer6)\n",
    "    layer7_ = BatchNormalization()(layer7)\n",
    "    layer7 = LeakyReLU()(layer7_)\n",
    "\n",
    "\n",
    "\n",
    "    layer8 = Conv2DTranspose(filters=512,kernel_size=kernel_size,strides=2,padding='same',use_bias=False,kernel_initializer=initializer)(layer7)\n",
    "    layer8_ = BatchNormalization()(layer8)\n",
    "    layer8 = LeakyReLU()(layer8_)\n",
    "\n",
    "\n",
    "\n",
    "    layer9 = Conv2D(filters=512,kernel_size=kernel_size,strides=2,padding='same',kernel_initializer=initializer,use_bias=False)(layer8)\n",
    "    layer9 = BatchNormalization()(layer9)\n",
    "    layer9 = layer9+layer7_\n",
    "    layer9 = Dropout(drop_out)(layer9)\n",
    "    layer9 = ReLU()(layer9)\n",
    "\n",
    "    layer10 = Conv2D(filters=512,kernel_size=kernel_size,strides=2,padding='same',kernel_initializer=initializer,use_bias=False)(layer9)\n",
    "    layer10 = BatchNormalization()(layer10)\n",
    "    layer10 = concatenate([layer10,layer6_])\n",
    "    layer10 = Dropout(drop_out)(layer10)\n",
    "    layer10 = ReLU()(layer10)\n",
    "\n",
    "    layer11 = Conv2D(filters=512,kernel_size=kernel_size,strides=2,padding='same',kernel_initializer=initializer,use_bias=False)(layer10)\n",
    "    layer11 = BatchNormalization()(layer11)\n",
    "    layer11 = layer11+layer5_\n",
    "    layer11 = Dropout(drop_out)(layer11)\n",
    "    layer11 = ReLU()(layer11)\n",
    "\n",
    "    layer12 = Conv2D(filters=512,kernel_size=kernel_size,strides=2,padding='same',kernel_initializer=initializer,use_bias=False)(layer11)\n",
    "    layer12 = BatchNormalization()(layer12)\n",
    "    layer12 = layer12+layer4_\n",
    "    layer12 = ReLU()(layer12)\n",
    "\n",
    "    layer13 = Conv2D(filters=256,kernel_size=kernel_size,strides=2,padding='same',kernel_initializer=initializer,use_bias=False)(layer12)\n",
    "    layer13 = BatchNormalization()(layer13)\n",
    "    layer13 = layer13+layer3_\n",
    "    layer13 = ReLU()(layer13)\n",
    "\n",
    "    layer14 = Conv2D(filters=128,kernel_size=kernel_size,strides=2,padding='same',kernel_initializer=initializer,use_bias=False)(layer13)\n",
    "    layer14 = BatchNormalization()(layer14)\n",
    "    layer14 = layer14+layer2_\n",
    "    layer14 = ReLU()(layer14)\n",
    "\n",
    "    layer15 = Conv2D(filters=64,kernel_size=kernel_size,strides=2,padding='same',kernel_initializer=initializer,use_bias=False)(layer14)\n",
    "    layer15 = BatchNormalization()(layer15)\n",
    "    layer15 = layer15+layer1_\n",
    "    layer15 = ReLU()(layer15)\n",
    "\n",
    "    outputs = Conv2D(3,4,strides=2,padding='same',kernel_initializer=initializer,activation='tanh')(layer15)\n",
    "\n",
    "    Generator = Model(inputs=inputs,outputs=outputs)\n",
    "    return Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
